<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Xiaoqiang&#39;s Homepage</title>
    <link>http://xiaoqiangzhou.cn/post/</link>
      <atom:link href="http://xiaoqiangzhou.cn/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 16 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://xiaoqiangzhou.cn/media/icon_hudd244c7a401d7898701885a05ebe1cd4_6259_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>http://xiaoqiangzhou.cn/post/</link>
    </image>
    
    <item>
      <title>How to Make a Benchmark Analysis</title>
      <link>http://xiaoqiangzhou.cn/post/benchmark/</link>
      <pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://xiaoqiangzhou.cn/post/benchmark/</guid>
      <description>&lt;h3 id=&#34;cvpr-2019-single-image-deraining-a-comprehensive-benchmark-analysis&#34;&gt;[CVPR 2019] Single Image Deraining: A Comprehensive Benchmark Analysis&lt;/h3&gt;
&lt;h6 id=&#34;motivation&#34;&gt;Motivation:&lt;/h6&gt;
&lt;p&gt;It is thus unclear how these algorithms would perform on rainy images acquired â€œin the wildâ€ and how we could gauge the progress in the field.&lt;/p&gt;
&lt;h6 id=&#34;pipeline&#34;&gt;Pipeline:&lt;/h6&gt;
&lt;ol&gt;
&lt;li&gt;Formulate the rainy image synthesis process (from simple to complex)&lt;/li&gt;
&lt;li&gt;Summarize the contribution (thorough methods/testing sets)&lt;/li&gt;
&lt;li&gt;Method: Introduce the training sets&lt;/li&gt;
&lt;li&gt;Method: Introduce the testing sets&lt;/li&gt;
&lt;li&gt;Method: Introduce the additional task-driven (object detection in rainy images) testing sets.&lt;/li&gt;
&lt;li&gt;Experiments: Objective comparison (PSNR, SSIM, NIQE, SSEQ, and BLIINDSII)&lt;/li&gt;
&lt;li&gt;Experiments: Subjective comparison (11 human raters)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;tpami-2021-cross-domain-facial-expression-recognition-a-unified-evaluation-benchmark-and-adversarial-graph-learning&#34;&gt;[TPAMI 2021] Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning&lt;/h3&gt;
&lt;h6 id=&#34;motivation-1&#34;&gt;Motivation:&lt;/h6&gt;
&lt;ol&gt;
&lt;li&gt;Comprehensive and fair comparisons are lacking due to inconsistent choices of the source/target datasets and feature extractors.&lt;/li&gt;
&lt;li&gt;This journal paper is an extension work on their conference paper, which is published on ACM MM 2020. Therefore, the authors propose a novel method for this task.&lt;/li&gt;
&lt;/ol&gt;
&lt;h6 id=&#34;pipeline-1&#34;&gt;Pipeline:&lt;/h6&gt;
&lt;ol&gt;
&lt;li&gt;Category existing works by the training sets and backbone network.&lt;/li&gt;
&lt;li&gt;Build a simple baseline with ResNet-50 and take experiments to verify the influence of domain gap caused by the selected training sets.&lt;/li&gt;
&lt;li&gt;Conduct experiments to compare the effectiveness of different backbones under a baseline design.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Re-implement some SOTA methods manually and take thorough experiments with different training sets / backbone.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Evaluation Protocols: Dataset choice/Feature extractor choice&lt;/li&gt;
&lt;li&gt;Introduce the proposed method.&lt;/li&gt;
&lt;li&gt;Ablation study on the proposed method.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;tpami-2020-toward-bridging-the-simulated-to-real-gap-benchmarking-super-resolution-on-real-data&#34;&gt;[TPAMI 2020] Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution on Real Data&lt;/h3&gt;
&lt;h3 id=&#34;cvpr-2020-cross-domain-document-object-detection-benchmark-suite-and-method&#34;&gt;[CVPR 2020] Cross-Domain Document Object Detection: Benchmark Suite and Method&lt;/h3&gt;
&lt;h3 id=&#34;wacv-2021-adaptiope-a-modern-benchmark-for-unsupervised-domain-adaptation&#34;&gt;[WACV 2021] Adaptiope: A Modern Benchmark for Unsupervised Domain Adaptation&lt;/h3&gt;
&lt;h3 id=&#34;todo--the-first-work-that-defines-the-setting-of-domain-generalization-task&#34;&gt;[TODO ] The first work that defines the setting of domain generalization task&lt;/h3&gt;
&lt;h3 id=&#34;submitted-to-neurips-21-ood-bench-benchmarking-and-understanding-out-of-distribution-generalization-datasets-and-algorithms&#34;&gt;[submitted to NeurIPS 21] OoD-Bench Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms&lt;/h3&gt;
&lt;h3 id=&#34;domain-generalization-in-vision-a-survey&#34;&gt;Domain Generalization in Vision: A Survey&lt;/h3&gt;
&lt;h3 id=&#34;number-of-paper-titles-including-benchmark&#34;&gt;Number of paper titles including &amp;ldquo;benchmark&amp;rdquo;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;ICCV 2021:&lt;/em&gt; 18. &lt;a href=&#34;https://openaccess.thecvf.com/ICCV2021?day=all&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CVPR 2021:&lt;/em&gt; 17. &lt;a href=&#34;https://openaccess.thecvf.com/CVPR2021?day=all&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ECCV 2020:&lt;/em&gt; 7.  &lt;a href=&#34;https://www.ecva.net/papers.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CVPR 2020:&lt;/em&gt; 6.  &lt;a href=&#34;https://openaccess.thecvf.com/CVPR2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;TPAMI&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;TPAMI&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: &lt;b&gt;Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution on Real Data&lt;/b&gt;
   Authors: TPAMI 2020
&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;ICCV 2021&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models
   Authors: Jingjing Liu
2. Title: Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments
    Authors: Ziyi Bai, Xilin Chen
3. &lt;b&gt;Title: FloW: A Dataset and Benchmark for Floating Waste Detection in Inland Waters&lt;/b&gt;
   Authors:  &lt;b&gt;Yoshua Bengio&lt;/b&gt;
4. Title: Webly Supervised Fine-Grained Recognition: Benchmark Datasets and an Approach
   Authors: Jian Zhang, Heng Tao Shen
5. Title: H2O: A Benchmark for Visual Human-Human Object Handover Analysis
   Authors: Yanfeng Wang, Cewu Lu
6. Title: RobustNav: Towards Benchmarking Robustness in Embodied Navigation
   Authors:
7. Title: Towards Real-World Prohibited Item Detection: A Large-Scale X-Ray Benchmark
   Authors:
8. &lt;b&gt;Title: Transparent Object Tracking Benchmark&lt;/b&gt;
   Authors: Haibin Ling
9. Title: E-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks
   Authors:
10. &lt;b&gt;Title: Benchmarking Ultra-High-Definition Image Super-Resolution&lt;/b&gt;
   Authors: Hongdong Li, Ming-Hsuan Yang
11. Title: Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation
   Authors:
12. &lt;b&gt;Title: Real-World Video Super-Resolution: A Benchmark Dataset and a Decomposition Based Learning Scheme&lt;/b&gt;
   Authors: Lei Zhang
13. &lt;b&gt;Title: HDR Video Reconstruction: A Coarse-To-Fine Network and a Real-World Benchmark Dataset&lt;/b&gt;
   Authors: Lei Zhang
&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;CVPR 2021&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: &lt;b&gt;Multi-Shot Temporal Event Localization: A Benchmark&lt;/b&gt;
   Authors: Xiang Bai, Philip H. S. Torr
2. Title: &lt;b&gt;Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline&lt;/b&gt;
   Authors:
3. Title: GMOT-40: A Benchmark for Generic Multiple Object Tracking
   Authors: Liang Lin
4. Title: ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis
   Authors: Ziwei Liu
&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;ECCV 2020&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: &lt;b&gt;Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms&lt;/b&gt;
   Authors: Jucheol Won, Sunghyun Cho
2. Title: &lt;b&gt;Towards causal benchmarking of bias in face analysis algorithms&lt;/b&gt;
   Authors: MIT, Amazon
&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;CVPR 2020&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: &lt;b&gt;Cross-Domain Document Object Detection: Benchmark Suite and Method&lt;/b&gt;
   Authors: Yun Fu
2. Title: &lt;b&gt;Supervised Raw Video Denoising With a Benchmark Dataset on Dynamic Scenes&lt;/b&gt;
   Authors: Ronghe Chu, Jingyu Yang
&lt;/pre&gt;
&lt;/details&gt;
</description>
    </item>
    
    <item>
      <title>[AAAI 2022] Revisiting Global Statistics Aggregation for Improving Image Restoration</title>
      <link>http://xiaoqiangzhou.cn/post/chu_revisiting/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://xiaoqiangzhou.cn/post/chu_revisiting/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/0_hubcb6207f291c8349ce1e6771be420ded_26203_50551a23622d6ba605ce93a2febed507.PNG 400w,
               /media/posts/Chu_revisiting/0_hubcb6207f291c8349ce1e6771be420ded_26203_655c2cf608a8f82a32a6ab53f2a4a0ad.PNG 760w,
               /media/posts/Chu_revisiting/0_hubcb6207f291c8349ce1e6771be420ded_26203_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/0_hubcb6207f291c8349ce1e6771be420ded_26203_50551a23622d6ba605ce93a2febed507.PNG&#34;
               width=&#34;760&#34;
               height=&#34;124&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Domain gap between training/testing statistics distribution. This paper shows that statistics aggregated on the patches-based/entire-image-based feature in the training/testing phase respectively may distribute very differently.&lt;/li&gt;
&lt;li&gt;The test-time solution can be summarized as converting the statistics aggregation operation from global to local, i.e. each pixel of the feature aggregates its own statistics locally.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;method&#34;&gt;Method:&lt;/h4&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-figure-1-illustration-of-training-and-testing-schemes-of-image-restoration&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/1_hucbeb4ea3d14f171cb4bad5478aff1a37_189404_fcf6783fd6dca99fdc5245eadc7f98c3.PNG 400w,
               /media/posts/Chu_revisiting/1_hucbeb4ea3d14f171cb4bad5478aff1a37_189404_7996b112f4293090ff5808333d721e00.PNG 760w,
               /media/posts/Chu_revisiting/1_hucbeb4ea3d14f171cb4bad5478aff1a37_189404_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/1_hucbeb4ea3d14f171cb4bad5478aff1a37_189404_fcf6783fd6dca99fdc5245eadc7f98c3.PNG&#34;
               width=&#34;444&#34;
               height=&#34;505&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1: Illustration of training and testing schemes of image restoration
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The computational complexity of local statistics for the whole image can be reduced from $O(HWK_hK_w)$ to $O(HW)$ with some mathematical tricks.
















&lt;figure  id=&#34;figure-figure-2-formulation-of-the-global-statistics-calculation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/2_hua655102713792929bb891bd08a988625_6675_245ad170589f238ce8ffb08eabaa987a.PNG 400w,
               /media/posts/Chu_revisiting/2_hua655102713792929bb891bd08a988625_6675_a766e4dccb6b8ea7b599f379155e73e1.PNG 760w,
               /media/posts/Chu_revisiting/2_hua655102713792929bb891bd08a988625_6675_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/2_hua655102713792929bb891bd08a988625_6675_245ad170589f238ce8ffb08eabaa987a.PNG&#34;
               width=&#34;267&#34;
               height=&#34;66&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 2: Formulation of the global statistics calculation
    &lt;/figcaption&gt;&lt;/figure&gt;

















&lt;figure  id=&#34;figure-figure-3-formulation-of-the-local-statistics-calculation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/3_hudfd7a9fccad84aae8d319dbc232eca76_6472_883f35f5566ea6f13dc276451848d99a.PNG 400w,
               /media/posts/Chu_revisiting/3_hudfd7a9fccad84aae8d319dbc232eca76_6472_66905c017bae1f8dcbf5e5c331759500.PNG 760w,
               /media/posts/Chu_revisiting/3_hudfd7a9fccad84aae8d319dbc232eca76_6472_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/3_hudfd7a9fccad84aae8d319dbc232eca76_6472_883f35f5566ea6f13dc276451848d99a.PNG&#34;
               width=&#34;303&#34;
               height=&#34;58&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 3: Formulation of the local statistics calculation
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;The authors extend the proposed modules to the  Squeeze and Excitation (SE) block and Instance Normalization (IN) block.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;experiments&#34;&gt;Experiments&lt;/h4&gt;
&lt;p&gt;The authors take experiments on image restoration task and semantic segmentation task.
















&lt;figure  id=&#34;figure-figure-4--visualization-of-the-statistics-mean-distribution-of-in-and-se-in-traintest-time&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/4_huacd4ec92d12c6dd417289ba5c18477d9_102123_f4aace952048351c3036dce33503929a.PNG 400w,
               /media/posts/Chu_revisiting/4_huacd4ec92d12c6dd417289ba5c18477d9_102123_2b08cb8e73d241b2bc6805d023211817.PNG 760w,
               /media/posts/Chu_revisiting/4_huacd4ec92d12c6dd417289ba5c18477d9_102123_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/4_huacd4ec92d12c6dd417289ba5c18477d9_102123_f4aace952048351c3036dce33503929a.PNG&#34;
               width=&#34;760&#34;
               height=&#34;549&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 4:  Visualization of the statistics (mean) distribution of IN and SE in train/test-time
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;some-take-away-conclusions&#34;&gt;Some take-away conclusions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Full-image training causes severe performance loss in low-level vision task.  This is explained by that full-images training lacks cropping augmentation&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Build up Your Homepage with Hugo</title>
      <link>http://xiaoqiangzhou.cn/post/hompage-buildup/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://xiaoqiangzhou.cn/post/hompage-buildup/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Build up the template homepage with Hugo themes.&lt;/li&gt;
&lt;li&gt;Revise the content with your personal information and perference.&lt;/li&gt;
&lt;li&gt;Add some new features beyond this template.&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
1. The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site
2. The template can be modified and customised to suit your needs. It&#39;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a **no-code solution (write in Markdown and customize with YAML parameters)** and having **flexibility to later add even deeper personalization with HTML and CSS**
3. You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more















&lt;figure  id=&#34;figure-the-template-is-mobile-first-with-a-responsive-design-to-ensure-that-your-site-looks-stunning-on-every-device&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://raw.githubusercontent.com/wowchemy/wowchemy-hugo-modules/master/academic.png&#34; alt=&#34;The template is mobile first with a responsive design to ensure that your site looks stunning on every device.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      The template is mobile first with a responsive design to ensure that your site looks stunning on every device.
    &lt;/figcaption&gt;&lt;/figure&gt;

## Get Started

- ğŸ‘‰ [**Create a new site**](https://wowchemy.com/templates/)
- ğŸ“š [**Personalize your site**](https://wowchemy.com/docs/)
- ğŸ’¬ [Chat with the **Wowchemy community**](https://discord.gg/z8wNYzb) or [**Hugo community**](https://discourse.gohugo.io)
- ğŸ¦ Twitter: [@wowchemy](https://twitter.com/wowchemy) [@GeorgeCushen](https://twitter.com/GeorgeCushen) [#MadeWithWowchemy](https://twitter.com/search?q=(%23MadeWithWowchemy%20OR%20%23MadeWithAcademic)&amp;src=typed_query)
- ğŸ’¡ [Request a **feature** or report a **bug** for _Wowchemy_](https://github.com/wowchemy/wowchemy-hugo-modules/issues)
- â¬†ï¸ **Updating Wowchemy?** View the [Update Tutorial](https://wowchemy.com/docs/hugo-tutorials/update/) and [Release Notes](https://wowchemy.com/updates/)

## Crowd-funded open-source software

To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.

### [â¤ï¸ Click here to become a sponsor and help support Wowchemy&#39;s future â¤ï¸](https://wowchemy.com/plans/)

As a token of appreciation for sponsoring, you can **unlock [these](https://wowchemy.com/plans/) awesome rewards and extra features ğŸ¦„âœ¨**

## Ecosystem

* **[Hugo Academic CLI](https://github.com/wowchemy/hugo-academic-cli):** Automatically import publications from BibTeX

## Inspiration

[Check out the latest **demo**](https://academic-demo.netlify.com/) of what you&#39;ll get in less than 10 minutes, or [view the **showcase**](https://wowchemy.com/user-stories/) of personal, project, and business sites.

## Features

- **Page builder** - Create *anything* with [**widgets**](https://wowchemy.com/docs/page-builder/) and [**elements**](https://wowchemy.com/docs/content/writing-markdown-latex/)
- **Edit any type of content** - Blog posts, publications, talks, slides, projects, and more!
- **Create content** in [**Markdown**](https://wowchemy.com/docs/content/writing-markdown-latex/), [**Jupyter**](https://wowchemy.com/docs/import/jupyter/), or [**RStudio**](https://wowchemy.com/docs/install-locally/)
- **Plugin System** - Fully customizable [**color** and **font themes**](https://wowchemy.com/docs/customization/)
- **Display Code and Math** - Code highlighting and [LaTeX math](https://en.wikibooks.org/wiki/LaTeX/Mathematics) supported
- **Integrations** - [Google Analytics](https://analytics.google.com), [Disqus commenting](https://disqus.com), Maps, Contact Forms, and more!
- **Beautiful Site** - Simple and refreshing one page design
- **Industry-Leading SEO** - Help get your website found on search engines and social media
- **Media Galleries** - Display your images and videos with captions in a customizable gallery
- **Mobile Friendly** - Look amazing on every screen with a mobile friendly version of your site
- **Multi-language** - 34+ language packs including English, ä¸­æ–‡, and PortuguÃªs
- **Multi-user** - Each author gets their own profile page
- **Privacy Pack** - Assists with GDPR
- **Stand Out** - Bring your site to life with animation, parallax backgrounds, and scroll effects
- **One-Click Deployment** - No servers. No databases. Only files.

## Themes

Wowchemy and its templates come with **automatic day (light) and night (dark) mode** built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the [Demo](https://academic-demo.netlify.com/) to see it in action! Day/night mode can also be disabled by the site admin in `params.toml`.

[Choose a stunning **theme** and **font**](https://wowchemy.com/docs/customization) for your site. Themes are fully customizable.
--&gt;&lt;blockquote&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Code Notes</title>
      <link>http://xiaoqiangzhou.cn/post/code_notes/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://xiaoqiangzhou.cn/post/code_notes/</guid>
      <description>&lt;h4 id=&#34;æå‡æ•ˆç‡çš„å‘½ä»¤æŠ€å·§&#34;&gt;æå‡æ•ˆç‡çš„å‘½ä»¤/æŠ€å·§&lt;/h4&gt;
&lt;p&gt;aliaså‘½ä»¤
åœ¨~/.bashrcæ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å‘½ä»¤ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;alias data=&amp;quot;cd /data1/username/exp_data&amp;quot;
alias codes=&amp;quot;cd /data1/username/codes&amp;quot;
alias ac_torch=&amp;quot;conda activate pytorch1.5.0&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ä¹‹åä¾¿å¯é€šè¿‡dataå’Œcodeså‘½ä»¤ç›´æ¥è¿›å…¥å¯¹åº”è·¯å¾„ä¸‹&lt;/p&gt;
&lt;p&gt;å…³äº~/.bashrcæ–‡ä»¶ï¼Œå¯ä»¥ç†è§£ä¸ºæ¯æ¬¡ç™»é™†æ—¶è‡ªåŠ¨æ‰§è¡Œä¸€æ¬¡å…¶ä¸­çš„å†…å®¹&lt;/p&gt;
&lt;p&gt;ç›´æ¥åœ¨å‘½ä»¤è¡Œæ‰§è¡Œexportï¼Œ aliasç­‰å‘½ä»¤ï¼Œæœ‰æ•ˆæœŸä¸ºæœ¬æ¬¡ç™»å½•æœŸé—´ã€‚&lt;/p&gt;
&lt;p&gt;å¯ä»¥å°†è‡ªå®šä¹‰å‘½ä»¤å†™å…¥å¦ä¸€ä¸ªæ–‡ä»¶~/.my_cmdï¼Œç„¶ååœ¨~/.bashrcæœ€ååŠ å…¥&lt;/p&gt;
&lt;p&gt;&lt;code&gt;source ~/.my_cmd&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;aliaså‘½ä»¤ä¸å…‰å¯ä»¥åœ¨~/.bashrcå³linuxå¹³å°ä½¿ç”¨ï¼Œåœ¨git, cmderç­‰è½¯ä»¶ä¸­å‡å¯ä»¥è¿›è¡Œé…ç½®ã€‚&lt;/p&gt;
&lt;p&gt;å¸¸ç”¨çš„å‘½ä»¤æ¯”å¦‚ssh &lt;strong&gt;@&lt;/strong&gt;ï¼Œ tensorboardéƒ½å¯ä»¥é€šè¿‡aliaså‘½ä»¤ç®€åŒ–&lt;/p&gt;
&lt;h4 id=&#34;git&#34;&gt;Git&lt;/h4&gt;
&lt;p&gt;gitå¯ä»¥å¾ˆæ–¹ä¾¿çš„å®ç°å¤šæœºã€å¤šå¹³å°ä¹‹é—´çš„ä»£ç åŒæ­¥ã€‚
è¿™é‡Œå‡è®¾æœ¬åœ°ä½¿ç”¨ä¸€å°æœºå™¨ç¼–è¾‘ä»£ç ï¼Œåœ¨ä¸¤ä¸ªå¹³å°ï¼ˆgithubå’Œå…¬å¸çš„gitä»“åº“ï¼‰ï¼Œå¤šä¸ªæœºå™¨ä¸Šï¼ˆç»„é‡Œçš„Nå°æœºå™¨ï¼Œå…¬å¸çš„Nå°æœºå™¨ï¼‰è¿›è¡Œä»£ç åŒæ­¥ã€‚
åšæ³•ï¼šå‚è§å¦ä¸€ç¯‡æ–‡ç« ï¼š&lt;a href=&#34;https://zhuanlan.zhihu.com/p/226278333&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;å‘¨æ™“å¼ºï¼šGit Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;â€‹&lt;/p&gt;
&lt;h4 id=&#34;pip&#34;&gt;Pip&lt;/h4&gt;
&lt;p&gt;ä½¿ç”¨æ¸…åæºä¸‹è½½ï¼š
&lt;code&gt;pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;vs-code&#34;&gt;VS Code&lt;/h4&gt;
&lt;p&gt;è¿œç¨‹è¿æ¥æœåŠ¡å™¨ï¼šå‚è€ƒ
&lt;a href=&#34;https://blog.csdn.net/weixin_42096901/article/details/105062195&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;windowsä¸‹ä½¿ç”¨vscodeè¿œç¨‹è¿æ¥LinuxæœåŠ¡å™¨è¿›è¡Œå¼€å‘&lt;/a&gt;
â€‹&lt;/p&gt;
&lt;h4 id=&#34;pytorchå®‰è£…&#34;&gt;PyTorchå®‰è£…&lt;/h4&gt;
&lt;p&gt;é¦–å…ˆä½¿ç”¨nvidia-smiå‘½ä»¤æŸ¥çœ‹CUDAé©±åŠ¨ç‰ˆæœ¬ï¼Œæ¯”å¦‚10.2&lt;/p&gt;
&lt;p&gt;åœ¨PyTorchç½‘é¡µï¼ˆé“¾æ¥ï¼‰å¯»æ‰¾å¯¹åº”çš„å‘½ä»¤ï¼Œå»ºè®®ä½¿ç”¨condaå‘½ä»¤å®‰è£…ï¼ŒåŒæ—¶cudatoolkitç‰ˆæœ¬ä¸ä¸Šè¿°CUDAé©±åŠ¨ç‰ˆæœ¬ä¿æŒä¸€è‡´&lt;/p&gt;
&lt;p&gt;ä¾‹å¦‚ä½¿ç”¨nvidia-smiå‘½ä»¤åï¼Œæ˜¾ç¤ºç‰ˆæœ¬ä¸º10.2, åˆ™å®‰è£…pytorchçš„å‘½ä»¤ä¸º&lt;/p&gt;
&lt;p&gt;&lt;code&gt;conda install pytorch==1.5.0 torchvision==0.6.0 cudatoolkit=10.2 -c pytorch&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;æ³¨æ„è¦å®‰è£…cudatoolkitåŒ…ï¼Œä¸ç„¶å¯èƒ½å¯¼è‡´&lt;code&gt;torch.cuda.is_available()=False&lt;/code&gt;ã€‚&lt;/p&gt;
&lt;p&gt;æ³¨æ„ï¼šPyTorchçš„å®‰è£…ä¸æ¶‰åŠåˆ°cudaå’Œcudnnçš„é…ç½®ï¼Œå¯ä»¥è®¤ä¸ºåœ¨cudatoolkitä¸­å®ç°äº†ç±»ä¼¼åŠŸèƒ½&lt;/p&gt;
&lt;h4 id=&#34;tensorflowå®‰è£…&#34;&gt;Tensorflowå®‰è£…&lt;/h4&gt;
&lt;p&gt;Tensorflowçš„å®‰è£…çš„å¤§ä½“æ€è·¯ï¼šé…ç½®cudaå’Œcudnn, ç„¶ååˆ©ç”¨exportæ·»åŠ åˆ°è·¯å¾„ï¼Œä¹‹åå†å®‰è£…tensorflowã€‚&lt;/p&gt;
&lt;p&gt;ç›®å‰ä¸å¤ªç¡®å®šçš„æ˜¯ï¼Œä½ æƒ³è¦å®‰è£…çš„cudaç‰ˆæœ¬ä»¥åŠnvidia-smiæ˜¾ç¤ºçš„cudaç‰ˆæœ¬ä»¥åŠç³»ç»Ÿ/usr/localä¸‹çš„cudaç‰ˆæœ¬ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠæ˜¯å¦ä¼šå†²çªç­‰ã€‚&lt;/p&gt;
&lt;p&gt;é¦–å…ˆæ£€æŸ¥æœ¬æœºæ˜¯å¦å·²ç»é…ç½®å¥½cudaå’Œcudnnï¼š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;æ£€æŸ¥cudaå®‰è£…æƒ…å†µï¼šæ‰§è¡Œnvcc -Vå‘½ä»¤ï¼Œå¦‚æœæ˜¾ç¤ºå‘½ä»¤ä¸å­˜åœ¨ï¼Œè¿›è¡Œç¬¬2æ­¥ï¼›å¦‚æœæ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯ï¼Œè¿›è¡Œç¬¬3æ­¥ã€‚&lt;/li&gt;
&lt;li&gt;cudaå®‰è£…çš„ç¡®è®¤ï¼šè¿›å…¥/usr/local/cuda/binç›®å½•ä¸‹ï¼Œæ‰§è¡Œ&lt;code&gt;nvcc -V&lt;/code&gt;ã€‚å¦‚æœæ˜¾ç¤ºäº†ç‰ˆæœ¬ä¿¡æ¯ï¼Œè¯´æ˜cudaå®‰è£…äº†ï¼Œä½†æ˜¯æ²¡æœ‰åŠ å…¥ç³»ç»Ÿè·¯å¾„ï¼Œä¹‹åå¦‚æœåœ¨ç¬¬3æ­¥ç¡®è®¤cudnnä¹Ÿç¡®å®å®‰è£…å¥½äº†ï¼Œå¯ä»¥å°†cudaåŠ å…¥ç³»ç»Ÿè·¯å¾„ï¼Œè¿›å…¥ç¬¬3æ­¥ã€‚å¦‚æœ/usr/local/cuda*è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯´æ˜æ²¡æœ‰å®‰è£…cudaã€‚cudaéƒ½æ²¡æœ‰é…ç½®çš„è¯ï¼ŒcudnnåŸºæœ¬ä¹Ÿæ²¡æœ‰é…ç½®ï¼Œéæ­£å¸¸é€€å‡ºã€‚&lt;/li&gt;
&lt;li&gt;æ£€æŸ¥cudnnå®‰è£…æƒ…å†µï¼šåœ¨cudaç›®å½•ä¸‹ï¼Œæ‰§è¡Œ&lt;code&gt;cat /path/to/cuda/include/cudnn.h | grep CUDNNMAJOR -A5&lt;/code&gt;å¦‚æœæ˜¾ç¤ºå‡ºCUDNN_MAJORï¼ŒCUDNN_MAç­‰ä¿¡æ¯ï¼Œåˆ™è¯æ˜å®‰è£…æˆåŠŸã€‚è¯´æ˜æœ¬æœºå·²ç»é…ç½®å¥½cudaå’Œcudnnï¼Œæ­£å¸¸é€€å‡ºã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;cudaå’Œcudnnç¯å¢ƒå‡†å¤‡å¥½ä¹‹åï¼Œå³å¯å®‰è£…tensorflow&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pip install tensorflow-gpu==***&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;å¯ä»¥é€šè¿‡&lt;/p&gt;
&lt;p&gt;&lt;code&gt;import tensorflow as tf;tf.test.is_gpu_available()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;æ£€æŸ¥ï¼Œæ­£ç¡®çš„è¾“å‡ºç»“æœä¸ºï¼š 















&lt;figure  id=&#34;figure-å¯ä»¥ä»logä¸­çœ‹åˆ°gpuä¿¡æ¯&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/code_notes/1_hue7104633ab21bc6a823ef89ce84a2436_199856_0d54e6d0b08994821aad9326e5905609.png 400w,
               /media/posts/code_notes/1_hue7104633ab21bc6a823ef89ce84a2436_199856_2c8653a74068e04de82f90c07d7910e5.png 760w,
               /media/posts/code_notes/1_hue7104633ab21bc6a823ef89ce84a2436_199856_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/code_notes/1_hue7104633ab21bc6a823ef89ce84a2436_199856_0d54e6d0b08994821aad9326e5905609.png&#34;
               width=&#34;720&#34;
               height=&#34;231&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      å¯ä»¥ä»logä¸­çœ‹åˆ°GPUä¿¡æ¯
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;å¯ä»¥ä»logä¸­çœ‹åˆ°GPUä¿¡æ¯&lt;/p&gt;
&lt;h4 id=&#34;cudaå’Œcudnnçš„å®‰è£…&#34;&gt;cudaå’Œcudnnçš„å®‰è£…ï¼š&lt;/h4&gt;
&lt;p&gt;é¦–å…ˆå®‰è£…cudaã€‚cudaä»“åº“ç½‘å€ï¼š&lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit-archive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CUDA Toolkit Archive&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ç»™æ–‡ä»¶è¿è¡Œæƒé™ chmod + x &lt;strong&gt;.run, ç„¶åè¿è¡Œ./&lt;/strong&gt;.runï¼Œä¾æ¬¡é€‰æ‹©accept&amp;mdash;no&amp;mdash;yesï¼Œç„¶åå°†cudaå®‰è£…åˆ°ä½ çš„ç›®å½•ä¸‹ï¼ˆå»ºè®®æŒ‡å®šè‡ªå·±çš„ç›®å½•ï¼‰&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¸‹è½½cudnnã€‚cudnnä»“åº“ç½‘å€ï¼š&lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-archive&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cuDNN Archive&lt;/a&gt;
â€‹
éœ€è¦æ³¨å†ŒNVIDIAè´¦å·ï¼Œæ ¹æ®åç§°ï¼Œæ‰¾å¯¹åº”åˆšæ‰cudaç‰ˆæœ¬çš„cudnnå®‰è£…åŒ…ä¸‹è½½è§£å‹ã€‚&lt;code&gt;tar -xzvf cudnn-**-linux-x64-v**.tgz&lt;/code&gt;ï¼Œwin10ä¸‹ä¸‹è½½cudnnæ—¶åç¼€å¯èƒ½è¢«ä¿®æ”¹ï¼Œé‡å‘½åä¸º**.tgzå³å¯ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è¿›è¡Œéƒ¨åˆ†æ–‡ä»¶çš„æ‹·è´ï¼Œä»cudnnè§£å‹åçš„cudaæ–‡ä»¶å¤¹ï¼Œå¤åˆ¶åˆ°ä¹‹å‰çš„cudaæ–‡ä»¶å¤¹ä¸­ï¼š&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;cp cuda/include/cudnn.h /path/to/your_cuda/include/
cp cuda/lib64/libcudnn* /path/to/your_cuda/lib64
chmod a+r /path/to/your_cuda/include/cudnn.h /path/to/your_cuda/lib64/libcudnn*
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;æŸ¥çœ‹cudaå’Œcudnnå®‰è£…çŠ¶æ€ï¼šåˆ†åˆ«ä¸ºnvcc -Vå’Œ&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;cat /path/to/your_cuda/include/cudnn.h | grep CUDNN_MAJOR -A5&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;å¦‚æœè¾“å‡ºæ­£å¸¸ç‰ˆæœ¬å·ï¼Œåˆ™è¯æ˜cudaå’Œcudnnå®‰è£…æˆåŠŸã€‚
















&lt;figure  id=&#34;figure-nvcc--væŸ¥çœ‹å®‰è£…çš„cudaç‰ˆæœ¬&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/code_notes/2_hu10a71d56ebc020aed6deac0607066265_2849_674099db9e7f2daf6a4dd4b7df21d8c8.png 400w,
               /media/posts/code_notes/2_hu10a71d56ebc020aed6deac0607066265_2849_0951221c47d782fce6f2007e04edd9d1.png 760w,
               /media/posts/code_notes/2_hu10a71d56ebc020aed6deac0607066265_2849_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/code_notes/2_hu10a71d56ebc020aed6deac0607066265_2849_674099db9e7f2daf6a4dd4b7df21d8c8.png&#34;
               width=&#34;376&#34;
               height=&#34;65&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      nvcc -VæŸ¥çœ‹å®‰è£…çš„cudaç‰ˆæœ¬
    &lt;/figcaption&gt;&lt;/figure&gt;

















&lt;figure  id=&#34;figure-æŸ¥çœ‹cudnnç‰ˆæœ¬&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/code_notes/3_hu3df6c53d633e6ef5f11526114bfd95cc_3703_991e7aa0a141841806320be208277f76.png 400w,
               /media/posts/code_notes/3_hu3df6c53d633e6ef5f11526114bfd95cc_3703_a9051bef438c078a2b86e4e9644ff529.png 760w,
               /media/posts/code_notes/3_hu3df6c53d633e6ef5f11526114bfd95cc_3703_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/code_notes/3_hu3df6c53d633e6ef5f11526114bfd95cc_3703_991e7aa0a141841806320be208277f76.png&#34;
               width=&#34;465&#34;
               height=&#34;145&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      æŸ¥çœ‹cudnnç‰ˆæœ¬
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;å¦‚æœcudaå®‰è£…æˆåŠŸåï¼Œå³nvcc -Væ˜¾ç¤ºæ­£ç¡®ç‰ˆæœ¬å·ï¼Œä½†æ˜¯cudnnå´å¤±è´¥äº†ï¼Œå¯ä»¥å°è¯•é€‰ç”¨ç¨æ—§ç‰ˆæœ¬çš„cudnnè€Œéæœ€æ–°ç‰ˆ&lt;/p&gt;
&lt;p&gt;tensorflowæˆªæ­¢åˆ°1.15ç‰ˆæœ¬ï¼Œéƒ½ä¸æ”¯æŒcuda-10.2ï¼Œä¸ºäº†å®‰è£…tensorflowï¼Œéœ€è¦æ‰‹åŠ¨é…ç½®cuda-10.0ï¼Œå¹¶ä¿®æ”¹~/.bashrcæ–‡ä»¶ä¸­çš„&lt;code&gt;PATH&lt;/code&gt;å’Œ&lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt;å˜é‡ã€‚&lt;/p&gt;
&lt;h4 id=&#34;pyrenderå®‰è£…&#34;&gt;PyRenderå®‰è£…&lt;/h4&gt;
&lt;p&gt;windowså®‰è£…ï¼šç¼ºå°‘freetypeä¾èµ–ï¼Œå‘ç°æ˜¯å› ä¸ºæ²¡æœ‰å®‰è£…visual studio&lt;/p&gt;
&lt;h4 id=&#34;pytorch3då®‰è£…&#34;&gt;PyTorch3Då®‰è£…&lt;/h4&gt;
&lt;p&gt;PyTorch3Dæ˜¯Facebookå¼€æºçš„3Dè§†è§‰å·¥å…·åŒ…ï¼Œå®‰è£…æŒ‡å¼•ï¼š&lt;a href=&#34;https://github.com/facebookresearch/pytorch3d/blob/master/INSTALL.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;é“¾æ¥&lt;/a&gt;
windowsï¼ˆæ— cpuï¼‰å®‰è£…ï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼ˆéœ€è¦æå‰å®‰è£…å¥½torch, torchvisionç­‰å·¥å…·åŒ…ï¼‰&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/facebookresearch/pytorch3d.git
cd pytorch3d
pip install -e .
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;linuxå¸¸ç”¨å‘½ä»¤&#34;&gt;Linuxå¸¸ç”¨å‘½ä»¤&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;tmux&lt;/strong&gt;ï¼šå°†ç¨‹åºæ”¾åœ¨åå°æ‰§è¡Œï¼Œå³ä½¿æ¨å‡ºsshç™»å½•ï¼Œåªè¦æœåŠ¡å™¨æ²¡æœ‰å…³æœºï¼Œç¨‹åºå°±ä¸ä¼šç»ˆç«¯ã€‚&lt;/p&gt;
&lt;p&gt;åˆ›å»ºæ–°çš„çª—å£ &lt;code&gt;tmux new -s your_name&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;æš‚æ—¶é€€å‡ºå½“å‰çª—å£ &lt;code&gt;ctrl+b+d&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;ç»“æŸç»ˆæ­¢å½“å‰çª—å£ &lt;code&gt;exit&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;æŸ¥çœ‹å·²åˆ›å»ºçš„çª—å£ &lt;code&gt;tmux ls&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;è¿›å…¥ä¹‹å‰åˆ›å»ºçš„çª—å£ &lt;code&gt;tmux attach -t your_nameæˆ–tmux a -t your_name&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nohup&lt;/strong&gt;ï¼šä¹Ÿæ˜¯å°†ç¨‹åºæ”¾åœ¨åå°æ‰§è¡Œï¼Œåœ¨æ²¡æœ‰tmuxçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨nohupæ˜¯ä¸€ä¸ªé€€è€Œæ±‚å…¶æ¬¡çš„é€‰æ‹©&lt;/p&gt;
&lt;p&gt;æ‰§è¡Œä»»åŠ¡å¹¶æ”¾åˆ°åå° &lt;code&gt;nohup bash yourscript.sh &amp;gt; your_log.log 2&amp;gt;&amp;amp;1 &amp;amp;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;ä¹‹ååªèƒ½é€šè¿‡æŸ¥çœ‹logæ–‡ä»¶ï¼Œæ¥äº†è§£ç¨‹åºè¿è¡Œè¾“å‡ºæƒ…å†µã€‚ä¸èƒ½åƒtmuxé‚£æ ·è¿›å…¥çª—å£æŸ¥çœ‹ã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;topï¼Œhtopï¼Œps -aux&lt;/strong&gt;ï¼šè¿™ä¸‰ä¸ªéƒ½æ˜¯ç”¨æ¥æŸ¥çœ‹æœ¬æœºè¿è¡Œçš„ä»»åŠ¡çš„ï¼ŒæŒ‰éœ€å–ç”¨ã€‚&lt;code&gt;htop&lt;/code&gt;å’Œ&lt;code&gt;ps -aux&lt;/code&gt;å¯ä»¥æ¯”è¾ƒæ–¹ä¾¿çš„å®šä½åˆ°å…·ä½“çš„ä»»åŠ¡å¯¹åº”çš„PIDè¿›è¡Œï¼Œæ–¹ä¾¿è¿›è¡ŒKill&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;scp&lt;/strong&gt;ï¼šç”¨æ¥è¿›è¡Œæ•°æ®ä¼ è¾“ã€‚&lt;/p&gt;
&lt;p&gt;æœåŠ¡å™¨ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ï¼š&lt;code&gt;scp username@server_ip_address:æ–‡ä»¶åœ¨æœåŠ¡å™¨çš„è·¯å¾„  æ–‡ä»¶åœ¨æœ¬åœ°çš„ä¿å­˜è·¯å¾„&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;æœ¬æœºæ–‡ä»¶ä¼ é€’ç»™æœåŠ¡å™¨ï¼š &lt;code&gt;scp æ–‡ä»¶åœ¨æœ¬åœ°çš„ä¿å­˜è·¯å¾„  username@server_ip_address:æ–‡ä»¶åœ¨æœåŠ¡å™¨çš„è·¯å¾„&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;è¿æ¥æœåŠ¡å™¨éœ€è¦æŒ‡å®šç«¯å£æ—¶ï¼ŒåŠ å…¥-P ï¼ˆå¤§å†™ï¼‰å‚æ•°&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nvidia-smiï¼Œgpustat&lt;/strong&gt;ï¼šè¿™ä¸¤ä¸ªå‘½ä»¤éƒ½å¯ä»¥ç”¨æ¥æŸ¥çœ‹å½“å‰æœºå™¨çš„æ˜¾å¡å ç”¨æƒ…å†µï¼Œgpustatéœ€è¦ä½¿ç”¨pipè¿›è¡Œå®‰è£…ï¼Œç•Œé¢ç®€æ´å¥½çœ‹äº›ã€‚&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;tensorboard&lt;/strong&gt;ï¼šç”¨äºæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»“æœ&lt;/p&gt;
&lt;p&gt;åŸºæœ¬è®¾ç½®ä¸ºï¼š&lt;code&gt;tensorboard --logdir path_to_log --port your_port&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;ä¹‹åç”¨æµè§ˆå™¨æ‰“å¼€ http://localhost:your_portå³å¯ï¼Œé»˜è®¤ç«¯å£6006&lt;/p&gt;
&lt;p&gt;æ‰“å¼€æœåŠ¡å™¨ä¸Šçš„logæ–‡ä»¶&lt;/p&gt;
&lt;p&gt;è¿æ¥æœåŠ¡å™¨ï¼š&lt;code&gt;ssh -L æœ¬åœ°ç«¯å£:127.0.0.1:æœåŠ¡å™¨ç«¯å£ username@server_ip_address&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;åœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œï¼š&lt;code&gt;tensorboard --logdir path_to_log --port æœåŠ¡å™¨ç«¯å£&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;åœ¨æœ¬åœ°æµè§ˆå™¨æ‰“å¼€ï¼šhttp://localhost:æœ¬åœ°ç«¯å£&lt;/p&gt;
&lt;p&gt;tensorboardçš„ä½¿ç”¨ï¼Œéœ€è¦pipå®‰è£…tensorboardåº“ï¼Œå¦‚æœå®‰è£…å®Œä¹‹åè¿˜æ˜¯æ²¡æœ‰ï¼Œéœ€è¦å¯»æ‰¾å¯¹åº”çš„æ–‡ä»¶&lt;/p&gt;
&lt;p&gt;&lt;code&gt;alias tensorboard=&#39;python /home/username/anaconda3/envs/your_env_name/lib/python3.6/site-packages/tensorboard/main.py&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;which&lt;/p&gt;
&lt;p&gt;å¯ä»¥ç”¨äºæŸ¥çœ‹å½“å‰ä½¿ç”¨çš„å‘½ä»¤è·¯å¾„ï¼Œä¾‹å¦‚ï¼š&lt;code&gt;which nvcc&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;pytorchç¬”è®°&#34;&gt;PyTorchç¬”è®°&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;å…³äºdevice&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;DataParallelä¼šè‡ªåŠ¨å°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°å¯¹åº”çš„æ¨¡å‹deviceä¸Šï¼ŒéªŒè¯ä»£ç ï¼š&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def verify_device():
    class net(nn.Module):
        def __init__(self):
            super(net, self).__init__()
        
        def forward(self, inputs):
            print(&amp;quot;data&#39;s device in model: &amp;quot;, inputs.device)

    my_net = net()
    my_net = nn.DataParallel(my_net.cuda())  # my_net = my_net.cuda()
    inputs = torch.ones((16, 3, 256, 256))
    print(&amp;quot;data&#39;s device out model: &amp;quot;, inputs.device)
    my_net(inputs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;åˆ©ç”¨DataParallel åªå°†æ¨¡å‹æ”¾åœ¨GPUä¸Šï¼Œè€Œæ²¡æœ‰å°†è¾“å…¥æ”¾åœ¨GPUä¸Šï¼Œè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data&#39;s device out model:  cpu
data&#39;s device in model:  cuda:0
data&#39;s device in model:  cuda:1
data&#39;s device in model:  cuda:2
data&#39;s device in model:  cuda:3
data&#39;s device in model:  cuda:5
data&#39;s device in model:  cuda:6
data&#39;s device in model:  cuda:7
data&#39;s device in model:  cuda:4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;å¦‚æœä½¿ç”¨æ³¨é‡Šä¸­çš„&lt;code&gt;my_net = my_net.cuda()&lt;/code&gt;ï¼Œåˆ™å¾—åˆ°&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data&#39;s device out model:  cpu
data&#39;s device in model:  cpu
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;bugè®°å½•&#34;&gt;BUGè®°å½•&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;PyTorchçš„dataloaderä¸€æ—¦æ‰§è¡Œ&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;for iter, batch in enumerate(dataloader):&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;ä¼šå¡ä½å¾ˆä¹…ï¼Œè¿›å…¥&lt;code&gt;__getitem__&lt;/code&gt;è¿›è¡Œdebugå‘ç°ä¸€ç›´åœ¨è¯»å–å›¾ç‰‡ï¼Œè™½ç„¶æˆ‘ä½¿ç”¨çš„å¹¶ä¸æ˜¯opencvåº“è¿›è¡Œæ•°æ®è¯»å–&lt;/p&gt;
&lt;p&gt;è§£å†³åŠæ³•ï¼šåœ¨ä¸»ç¨‹åºä¸­åŠ å…¥&lt;code&gt;cv2.setNumThreads(0)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;è°·æ­Œæœç´¢å…³é”®å­—ï¼špytorch dataloader stucks&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Tensorflowå¤šæ¬¡loadæ¨¡å‹(ä¸ºäº†è¿›è¡Œå¤šæ¬¡æµ‹è¯•)æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;ValueError: Variable *** already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? &lt;/code&gt;&lt;/p&gt;
&lt;p&gt;è¿™ç§æƒ…å†µï¼Œé€šå¸¸æ˜¯éœ€è¦åœ¨ä»£ç ä¸­åŠ å…¥ï¼Œæ¯æ¬¡loadæ¨¡å‹å‰æ¸…ç†ä¸€æ¬¡è®¡ç®—å›¾&lt;code&gt;tf.reset_default_graph()&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;åŸºäºPyTorchçš„ä»£ç ï¼Œè®­ç»ƒæ•ˆæœè¿˜ä¸é”™ï¼Œæµ‹è¯•æ•ˆæœå¾ˆç³Ÿç³•ï¼Œæ’é™¤è¿‡æ‹Ÿåˆçš„åŸå› ä»¥åŠæµ‹è¯•ä»£ç çš„é—®é¢˜ã€‚æœ€åå‘ç°æ˜¯è¯¥ä»£ç åœ¨æµ‹è¯•æ—¶éœ€è¦æŒ‡å®š&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;model.train()&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;ç¨‹åºæŠ¥é”™ï¼š&lt;code&gt;cudnn_status_not_initialized&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;å¯èƒ½æ˜¯å› ä¸ºå½“å‰æ˜¾å¡æ­£åœ¨è¢«å…¶ä»–ä»»åŠ¡å ç”¨&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;å®‰è£…neural_renderer_pytorch&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;RuntimeError: Error compiling objects for extension&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;è§£å†³åŠæ³•ï¼šå°†pytorchçš„ç‰ˆæœ¬ä»1.5é™åˆ°1.2&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;å¤šå¡GPUè®­ç»ƒï¼Œä¿å­˜çš„ä¹Ÿæ˜¯å¤šå¡æ¨¡å‹ï¼Œå´éœ€è¦å•å¡æµ‹è¯•&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;Unexpected key(s) in state_dict: &amp;quot;module.*&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;è§£å†³åŠæ³•ï¼š&lt;/p&gt;
&lt;p&gt;&lt;code&gt;model.load_state_dict({k.replace(&#39;module.&#39;, &#39;&#39;): v for k, v in torch.load(model_state_path).items()})&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;æ¨¡å‹åœ¨GPUä¸Šè®­ç»ƒï¼Œéœ€è¦åœ¨CPUä¸Šæµ‹è¯•ï¼Œè·‘inference&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;è§£å†³åŠæ³•ï¼š&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ckpt = torch.load(model_path, map_location=torch.device(&amp;quot;cpu&amp;quot;))&lt;/code&gt;&lt;/p&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;ç¨‹åºæŠ¥é”™: &lt;code&gt;ModuleNotFoundError: No module named â€˜networkâ€™&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;åŸå› ï¼šä¿å­˜æ¨¡å‹æ—¶ä½¿ç”¨çš„æ˜¯&lt;code&gt;torch.save(model)&lt;/code&gt; è€Œä¸æ˜¯&lt;code&gt;torch.save(model.state_dict)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;è§£å†³åŠæ³•ï¼šåœ¨æ‰§è¡Œæ¨¡å‹ä¿å­˜å‘½ä»¤çš„pyæ–‡ä»¶è·¯å¾„ä¸‹ï¼Œå†æ¬¡è¯»å–æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨&lt;code&gt;torch.save(model.state_dict)&lt;/code&gt;æ¥ä¿å­˜æ¨¡å‹&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
