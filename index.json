[{"authors":null,"categories":null,"content":"I am Xiaoqiang Zhou (å‘¨æ™“å¼º), a third-year Ph.D student in the University of Science and Technology of China (USTC). Iâ€™m under the co-supervision of Prof. Tieniu Tan (è°­é“ç‰›), Prof. Ran He (èµ«ç„¶) and Prof. Zilei Wang (ç‹å­ç£Š). Before that, I received my bachelor degree in Electronic Information Engineering from University of Science and Technology of China in June 2019.\n  Download my CV/resumÃ©.\n","date":1639612800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1638316800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://xiaoqiangzhou.cn/author/xiaoqiang-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/xiaoqiang-zhou/","section":"authors","summary":"I am Xiaoqiang Zhou (å‘¨æ™“å¼º), a third-year Ph.D student in the University of Science and Technology of China (USTC). Iâ€™m under the co-supervision of Prof. Tieniu Tan (è°­é“ç‰›), Prof. Ran He (èµ«ç„¶) and Prof.","tags":null,"title":"Xiaoqiang Zhou","type":"authors"},{"authors":null,"categories":null,"content":"å³æ©é” is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"http://xiaoqiangzhou.cn/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"å³æ©é” is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"å³æ©é”","type":"authors"},{"authors":null,"categories":null,"content":"Well\u0026hellip;This page is still very ugly.\n","date":1639699200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1639699200,"objectID":"88978152a6aea1d36650bde8bc0c4feb","permalink":"http://xiaoqiangzhou.cn/life/some_photos/","publishdate":"2021-12-17T00:00:00Z","relpermalink":"/life/some_photos/","section":"life","summary":"Some scene photos captured with my mobile phone.","tags":null,"title":"Photos","type":"book"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Xiaoqiang Zhou FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"http://xiaoqiangzhou.cn/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"ğŸ“Š Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"      ","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633046400,"objectID":"961f31c552f5e01fd045374b271d6c9a","permalink":"http://xiaoqiangzhou.cn/life/some_photos/beijing/","publishdate":"2021-10-01T00:00:00Z","relpermalink":"/life/some_photos/beijing/","section":"life","summary":"      ","tags":null,"title":"åŒ—äº¬","type":"book"},{"authors":null,"categories":null,"content":"       ","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"d06cd557ce938e9afa5a1d46a79c37c8","permalink":"http://xiaoqiangzhou.cn/life/some_photos/hangzhou/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/life/some_photos/hangzhou/","section":"life","summary":"       ","tags":null,"title":"æ­å·","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"http://xiaoqiangzhou.cn/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px\rdata_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;)\rfig = px.bar(data_canada, x='year', y='pop')\rfig.show()\r ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"http://xiaoqiangzhou.cn/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"http://xiaoqiangzhou.cn/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":null,"categories":null,"content":"   ","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"f067b139f1489f6b9b50457b4eb0bb3d","permalink":"http://xiaoqiangzhou.cn/life/some_photos/others/","publishdate":"2021-11-01T00:00:00Z","relpermalink":"/life/some_photos/others/","section":"life","summary":"   ","tags":null,"title":"å…¶ä»–","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"http://xiaoqiangzhou.cn/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Xiaoqiang Zhou"],"categories":["Paper notes"],"content":"[CVPR 2019] Single Image Deraining: A Comprehensive Benchmark Analysis Motivation: It is thus unclear how these algorithms would perform on rainy images acquired â€œin the wildâ€ and how we could gauge the progress in the field.\nPipeline:  Formulate the rainy image synthesis process (from simple to complex) Summarize the contribution (thorough methods/testing sets) Method: Introduce the training sets Method: Introduce the testing sets Method: Introduce the additional task-driven (object detection in rainy images) testing sets. Experiments: Objective comparison (PSNR, SSIM, NIQE, SSEQ, and BLIINDSII) Experiments: Subjective comparison (11 human raters)  [TPAMI 2021] Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning Motivation:  Comprehensive and fair comparisons are lacking due to inconsistent choices of the source/target datasets and feature extractors. This journal paper is an extension work on their conference paper, which is published on ACM MM 2020. Therefore, the authors propose a novel method for this task.  Pipeline:  Category existing works by the training sets and backbone network. Build a simple baseline with ResNet-50 and take experiments to verify the influence of domain gap caused by the selected training sets. Conduct experiments to compare the effectiveness of different backbones under a baseline design. Re-implement some SOTA methods manually and take thorough experiments with different training sets / backbone. Evaluation Protocols: Dataset choice/Feature extractor choice Introduce the proposed method. Ablation study on the proposed method.  [TPAMI 2020] Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution on Real Data [CVPR 2020] Cross-Domain Document Object Detection: Benchmark Suite and Method [WACV 2021] Adaptiope: A Modern Benchmark for Unsupervised Domain Adaptation [TODO ] The first work that defines the setting of domain generalization task [submitted to NeurIPS 21] OoD-Bench Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms Domain Generalization in Vision: A Survey Number of paper titles including \u0026ldquo;benchmark\u0026rdquo;:  ICCV 2021: 18. Link CVPR 2021: 17. Link ECCV 2020: 7. Link CVPR 2020: 6. Link TPAMI  \rTPAMI\r\r1. Title: Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution on Real Data\rAuthors: TPAMI 2020\r\r\r\rICCV 2021\r\r1. Title: Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models\rAuthors: Jingjing Liu\r2. Title: Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments\rAuthors: Ziyi Bai, Xilin Chen\r3. Title: FloW: A Dataset and Benchmark for Floating Waste Detection in Inland Waters\rAuthors: Yoshua Bengio\r4. Title: Webly Supervised Fine-Grained Recognition: Benchmark Datasets and an Approach\rAuthors: Jian Zhang, Heng Tao Shen\r5. Title: H2O: A Benchmark for Visual Human-Human Object Handover Analysis\rAuthors: Yanfeng Wang, Cewu Lu\r6. Title: RobustNav: Towards Benchmarking Robustness in Embodied Navigation\rAuthors:\r7. Title: Towards Real-World Prohibited Item Detection: A Large-Scale X-Ray Benchmark\rAuthors:\r8. Title: Transparent Object Tracking Benchmark\rAuthors: Haibin Ling\r9. Title: E-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks\rAuthors:\r10. Title: Benchmarking Ultra-High-Definition Image Super-Resolution\rAuthors: Hongdong Li, Ming-Hsuan Yang\r11. Title: Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation\rAuthors:\r12. Title: Real-World Video Super-Resolution: A Benchmark Dataset and a Decomposition Based Learning Scheme\rAuthors: Lei Zhang\r13. Title: HDR Video Reconstruction: A Coarse-To-Fine Network and a Real-World Benchmark Dataset\rAuthors: Lei Zhang\r\r\r\rCVPR 2021\r\r1. Title: Multi-Shot Temporal Event Localization: A Benchmark\rAuthors: Xiang Bai, Philip H. S. Torr\r2. Title: Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline\rAuthors:\r3. Title: GMOT-40: A Benchmark for Generic Multiple Object Tracking\rAuthors: Liang Lin\r4. Title: ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis\rAuthors: Ziwei Liu\r\r\r\rECCV 2020\r\r1. Title: Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms\rAuthors: Jucheol Won, Sunghyun Cho\r2. Title: Towards causal benchmarking of bias in face analysis algorithms\rAuthors: MIT, Amazon\r\r\r\rCVPR 2020\r\r1. Title: Cross-Domain Document Object Detection: Benchmark Suite and Method\rAuthors: Yun Fu\r2. Title: Supervised Raw Video Denoising With a Benchmark Dataset on Dynamic Scenes\rAuthors: Ronghe Chu, Jingyu Yang\r\r\r","date":1639612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608076800,"objectID":"50c12c8612f0aa29279c992b116d9454","permalink":"http://xiaoqiangzhou.cn/post/benchmark/","publishdate":"2021-12-16T00:00:00Z","relpermalink":"/post/benchmark/","section":"post","summary":"List some papers that propose a benchmark in their areas.","tags":["Benchmark"],"title":"How to Make a Benchmark Analysis","type":"post"},{"authors":["Xiaoqiang Zhou"],"categories":["Paper notes"],"content":"    Summary:  Domain gap between training/testing statistics distribution. This paper shows that statistics aggregated on the patches-based/entire-image-based feature in the training/testing phase respectively may distribute very differently. The test-time solution can be summarized as converting the statistics aggregation operation from global to local, i.e. each pixel of the feature aggregates its own statistics locally.  Method:    Figure 1: Illustration of training and testing schemes of image restoration   The computational complexity of local statistics for the whole image can be reduced from $O(HWK_hK_w)$ to $O(HW)$ with some mathematical tricks.   Figure 2: Formulation of the global statistics calculation    Figure 3: Formulation of the local statistics calculation   The authors extend the proposed modules to the Squeeze and Excitation (SE) block and Instance Normalization (IN) block.  Experiments The authors take experiments on image restoration task and semantic segmentation task.   Figure 4: Visualization of the statistics (mean) distribution of IN and SE in train/test-time  Some take-away conclusions  Full-image training causes severe performance loss in low-level vision task. This is explained by that full-images training lacks cropping augmentation  ","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"9a638d49acf97e644c46e85570fb7188","permalink":"http://xiaoqiangzhou.cn/post/chu_revisiting/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/post/chu_revisiting/","section":"post","summary":"This paper firstly shows that statistics aggregated on the patches-based/entire-image-based feature in the training/testing phase respectively may distribute very differently.","tags":["Image Restoration","Feature Aggregation","Megvii"],"title":"[AAAI 2022] Revisiting Global Statistics Aggregation for Improving Image Restoration","type":"post"},{"authors":["Xiaoqiang Zhou"],"categories":["Tutorials"],"content":"Overview  Build up the template homepage with Hugo themes. Revise the content with your personal information and perference. Add some new features beyond this template.    The template is mobile first with a responsive design to ensure that your site looks stunning on every device. \r## Get Started\r- ğŸ‘‰ [**Create a new site**](https://wowchemy.com/templates/)\r- ğŸ“š [**Personalize your site**](https://wowchemy.com/docs/)\r- ğŸ’¬ [Chat with the **Wowchemy community**](https://discord.gg/z8wNYzb) or [**Hugo community**](https://discourse.gohugo.io)\r- ğŸ¦ Twitter: [@wowchemy](https://twitter.com/wowchemy) [@GeorgeCushen](https://twitter.com/GeorgeCushen) [#MadeWithWowchemy](https://twitter.com/search?q=(%23MadeWithWowchemy%20OR%20%23MadeWithAcademic)\u0026src=typed_query)\r- ğŸ’¡ [Request a **feature** or report a **bug** for _Wowchemy_](https://github.com/wowchemy/wowchemy-hugo-modules/issues)\r- â¬†ï¸ **Updating Wowchemy?** View the [Update Tutorial](https://wowchemy.com/docs/hugo-tutorials/update/) and [Release Notes](https://wowchemy.com/updates/)\r## Crowd-funded open-source software\rTo help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\r### [â¤ï¸ Click here to become a sponsor and help support Wowchemy's future â¤ï¸](https://wowchemy.com/plans/)\rAs a token of appreciation for sponsoring, you can **unlock [these](https://wowchemy.com/plans/) awesome rewards and extra features ğŸ¦„âœ¨**\r## Ecosystem\r* **[Hugo Academic CLI](https://github.com/wowchemy/hugo-academic-cli):** Automatically import publications from BibTeX\r## Inspiration\r[Check out the latest **demo**](https://academic-demo.netlify.com/) of what you'll get in less than 10 minutes, or [view the **showcase**](https://wowchemy.com/user-stories/) of personal, project, and business sites.\r## Features\r- **Page builder** - Create *anything* with [**widgets**](https://wowchemy.com/docs/page-builder/) and [**elements**](https://wowchemy.com/docs/content/writing-markdown-latex/)\r- **Edit any type of content** - Blog posts, publications, talks, slides, projects, and more!\r- **Create content** in [**Markdown**](https://wowchemy.com/docs/content/writing-markdown-latex/), [**Jupyter**](https://wowchemy.com/docs/import/jupyter/), or [**RStudio**](https://wowchemy.com/docs/install-locally/)\r- **Plugin System** - Fully customizable [**color** and **font themes**](https://wowchemy.com/docs/customization/)\r- **Display Code and Math** - Code highlighting and [LaTeX math](https://en.wikibooks.org/wiki/LaTeX/Mathematics) supported\r- **Integrations** - [Google Analytics](https://analytics.google.com), [Disqus commenting](https://disqus.com), Maps, Contact Forms, and more!\r- **Beautiful Site** - Simple and refreshing one page design\r- **Industry-Leading SEO** - Help get your website found on search engines and social media\r- **Media Galleries** - Display your images and videos with captions in a customizable gallery\r- **Mobile Friendly** - Look amazing on every screen with a mobile friendly version of your site\r- **Multi-language** - 34+ language packs including English, ä¸­æ–‡, and PortuguÃªs\r- **Multi-user** - Each author gets their own profile page\r- **Privacy Pack** - Assists with GDPR\r- **Stand Out** - Bring your site to life with animation, parallax backgrounds, and scroll effects\r- **One-Click Deployment** - No servers. No databases. Only files.\r## Themes\rWowchemy and its templates come with **automatic day (light) and night (dark) mode** built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the [Demo](https://academic-demo.netlify.com/) to see it in action! Day/night mode can also be disabled by the site admin in `params.toml`.\r[Choose a stunning **theme** and **font**](https://wowchemy.com/docs/customization) for your site. Themes are fully customizable.\r--  ","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606867200,"objectID":"91ff51c9bb59fe2a9046f19d650e82ff","permalink":"http://xiaoqiangzhou.cn/post/hompage-buildup/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/post/hompage-buildup/","section":"post","summary":"I write this post to record the way to build up my homepage with the Hugo tools.","tags":["Website","Hugo"],"title":"Build up Your Homepage with Hugo","type":"post"},{"authors":["Xiaoqiang Zhou"],"categories":["Tutorials"],"content":"æå‡æ•ˆç‡çš„å‘½ä»¤/æŠ€å·§ aliaså‘½ä»¤ åœ¨~/.bashrcæ–‡ä»¶ä¸­æ·»åŠ å¦‚ä¸‹å‘½ä»¤ï¼š\nalias data=\u0026quot;cd /data1/username/exp_data\u0026quot;\ralias codes=\u0026quot;cd /data1/username/codes\u0026quot;\ralias ac_torch=\u0026quot;conda activate pytorch1.5.0\u0026quot;\r ä¹‹åä¾¿å¯é€šè¿‡dataå’Œcodeså‘½ä»¤ç›´æ¥è¿›å…¥å¯¹åº”è·¯å¾„ä¸‹\nå…³äº~/.bashrcæ–‡ä»¶ï¼Œå¯ä»¥ç†è§£ä¸ºæ¯æ¬¡ç™»é™†æ—¶è‡ªåŠ¨æ‰§è¡Œä¸€æ¬¡å…¶ä¸­çš„å†…å®¹\nç›´æ¥åœ¨å‘½ä»¤è¡Œæ‰§è¡Œexportï¼Œ aliasç­‰å‘½ä»¤ï¼Œæœ‰æ•ˆæœŸä¸ºæœ¬æ¬¡ç™»å½•æœŸé—´ã€‚\nå¯ä»¥å°†è‡ªå®šä¹‰å‘½ä»¤å†™å…¥å¦ä¸€ä¸ªæ–‡ä»¶~/.my_cmdï¼Œç„¶ååœ¨~/.bashrcæœ€ååŠ å…¥\nsource ~/.my_cmd\naliaså‘½ä»¤ä¸å…‰å¯ä»¥åœ¨~/.bashrcå³linuxå¹³å°ä½¿ç”¨ï¼Œåœ¨git, cmderç­‰è½¯ä»¶ä¸­å‡å¯ä»¥è¿›è¡Œé…ç½®ã€‚\nå¸¸ç”¨çš„å‘½ä»¤æ¯”å¦‚ssh @ï¼Œ tensorboardéƒ½å¯ä»¥é€šè¿‡aliaså‘½ä»¤ç®€åŒ–\nGit gitå¯ä»¥å¾ˆæ–¹ä¾¿çš„å®ç°å¤šæœºã€å¤šå¹³å°ä¹‹é—´çš„ä»£ç åŒæ­¥ã€‚ è¿™é‡Œå‡è®¾æœ¬åœ°ä½¿ç”¨ä¸€å°æœºå™¨ç¼–è¾‘ä»£ç ï¼Œåœ¨ä¸¤ä¸ªå¹³å°ï¼ˆgithubå’Œå…¬å¸çš„gitä»“åº“ï¼‰ï¼Œå¤šä¸ªæœºå™¨ä¸Šï¼ˆç»„é‡Œçš„Nå°æœºå™¨ï¼Œå…¬å¸çš„Nå°æœºå™¨ï¼‰è¿›è¡Œä»£ç åŒæ­¥ã€‚ åšæ³•ï¼šå‚è§å¦ä¸€ç¯‡æ–‡ç« ï¼šå‘¨æ™“å¼ºï¼šGit Tutorial\nâ€‹\nPip ä½¿ç”¨æ¸…åæºä¸‹è½½ï¼š pip install -i https://pypi.tuna.tsinghua.edu.cn/simple some-package\nVS Code è¿œç¨‹è¿æ¥æœåŠ¡å™¨ï¼šå‚è€ƒ windowsä¸‹ä½¿ç”¨vscodeè¿œç¨‹è¿æ¥LinuxæœåŠ¡å™¨è¿›è¡Œå¼€å‘ â€‹\nPyTorchå®‰è£… é¦–å…ˆä½¿ç”¨nvidia-smiå‘½ä»¤æŸ¥çœ‹CUDAé©±åŠ¨ç‰ˆæœ¬ï¼Œæ¯”å¦‚10.2\nåœ¨PyTorchç½‘é¡µï¼ˆé“¾æ¥ï¼‰å¯»æ‰¾å¯¹åº”çš„å‘½ä»¤ï¼Œå»ºè®®ä½¿ç”¨condaå‘½ä»¤å®‰è£…ï¼ŒåŒæ—¶cudatoolkitç‰ˆæœ¬ä¸ä¸Šè¿°CUDAé©±åŠ¨ç‰ˆæœ¬ä¿æŒä¸€è‡´\nä¾‹å¦‚ä½¿ç”¨nvidia-smiå‘½ä»¤åï¼Œæ˜¾ç¤ºç‰ˆæœ¬ä¸º10.2, åˆ™å®‰è£…pytorchçš„å‘½ä»¤ä¸º\nconda install pytorch==1.5.0 torchvision==0.6.0 cudatoolkit=10.2 -c pytorch\næ³¨æ„è¦å®‰è£…cudatoolkitåŒ…ï¼Œä¸ç„¶å¯èƒ½å¯¼è‡´torch.cuda.is_available()=Falseã€‚\næ³¨æ„ï¼šPyTorchçš„å®‰è£…ä¸æ¶‰åŠåˆ°cudaå’Œcudnnçš„é…ç½®ï¼Œå¯ä»¥è®¤ä¸ºåœ¨cudatoolkitä¸­å®ç°äº†ç±»ä¼¼åŠŸèƒ½\nTensorflowå®‰è£… Tensorflowçš„å®‰è£…çš„å¤§ä½“æ€è·¯ï¼šé…ç½®cudaå’Œcudnn, ç„¶ååˆ©ç”¨exportæ·»åŠ åˆ°è·¯å¾„ï¼Œä¹‹åå†å®‰è£…tensorflowã€‚\nç›®å‰ä¸å¤ªç¡®å®šçš„æ˜¯ï¼Œä½ æƒ³è¦å®‰è£…çš„cudaç‰ˆæœ¬ä»¥åŠnvidia-smiæ˜¾ç¤ºçš„cudaç‰ˆæœ¬ä»¥åŠç³»ç»Ÿ/usr/localä¸‹çš„cudaç‰ˆæœ¬ä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠæ˜¯å¦ä¼šå†²çªç­‰ã€‚\né¦–å…ˆæ£€æŸ¥æœ¬æœºæ˜¯å¦å·²ç»é…ç½®å¥½cudaå’Œcudnnï¼š\n æ£€æŸ¥cudaå®‰è£…æƒ…å†µï¼šæ‰§è¡Œnvcc -Vå‘½ä»¤ï¼Œå¦‚æœæ˜¾ç¤ºå‘½ä»¤ä¸å­˜åœ¨ï¼Œè¿›è¡Œç¬¬2æ­¥ï¼›å¦‚æœæ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯ï¼Œè¿›è¡Œç¬¬3æ­¥ã€‚ cudaå®‰è£…çš„ç¡®è®¤ï¼šè¿›å…¥/usr/local/cuda/binç›®å½•ä¸‹ï¼Œæ‰§è¡Œnvcc -Vã€‚å¦‚æœæ˜¾ç¤ºäº†ç‰ˆæœ¬ä¿¡æ¯ï¼Œè¯´æ˜cudaå®‰è£…äº†ï¼Œä½†æ˜¯æ²¡æœ‰åŠ å…¥ç³»ç»Ÿè·¯å¾„ï¼Œä¹‹åå¦‚æœåœ¨ç¬¬3æ­¥ç¡®è®¤cudnnä¹Ÿç¡®å®å®‰è£…å¥½äº†ï¼Œå¯ä»¥å°†cudaåŠ å…¥ç³»ç»Ÿè·¯å¾„ï¼Œè¿›å…¥ç¬¬3æ­¥ã€‚å¦‚æœ/usr/local/cuda*è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯´æ˜æ²¡æœ‰å®‰è£…cudaã€‚cudaéƒ½æ²¡æœ‰é…ç½®çš„è¯ï¼ŒcudnnåŸºæœ¬ä¹Ÿæ²¡æœ‰é…ç½®ï¼Œéæ­£å¸¸é€€å‡ºã€‚ æ£€æŸ¥cudnnå®‰è£…æƒ…å†µï¼šåœ¨cudaç›®å½•ä¸‹ï¼Œæ‰§è¡Œcat /path/to/cuda/include/cudnn.h | grep CUDNNMAJOR -A5å¦‚æœæ˜¾ç¤ºå‡ºCUDNN_MAJORï¼ŒCUDNN_MAç­‰ä¿¡æ¯ï¼Œåˆ™è¯æ˜å®‰è£…æˆåŠŸã€‚è¯´æ˜æœ¬æœºå·²ç»é…ç½®å¥½cudaå’Œcudnnï¼Œæ­£å¸¸é€€å‡ºã€‚  cudaå’Œcudnnç¯å¢ƒå‡†å¤‡å¥½ä¹‹åï¼Œå³å¯å®‰è£…tensorflow\npip install tensorflow-gpu==***\nå¯ä»¥é€šè¿‡\nimport tensorflow as tf;tf.test.is_gpu_available()\næ£€æŸ¥ï¼Œæ­£ç¡®çš„è¾“å‡ºç»“æœä¸ºï¼š   å¯ä»¥ä»logä¸­çœ‹åˆ°GPUä¿¡æ¯  å¯ä»¥ä»logä¸­çœ‹åˆ°GPUä¿¡æ¯\ncudaå’Œcudnnçš„å®‰è£…ï¼š é¦–å…ˆå®‰è£…cudaã€‚cudaä»“åº“ç½‘å€ï¼šCUDA Toolkit Archive\n  ç»™æ–‡ä»¶è¿è¡Œæƒé™ chmod + x .run, ç„¶åè¿è¡Œ./.runï¼Œä¾æ¬¡é€‰æ‹©accept\u0026mdash;no\u0026mdash;yesï¼Œç„¶åå°†cudaå®‰è£…åˆ°ä½ çš„ç›®å½•ä¸‹ï¼ˆå»ºè®®æŒ‡å®šè‡ªå·±çš„ç›®å½•ï¼‰\n  ä¸‹è½½cudnnã€‚cudnnä»“åº“ç½‘å€ï¼šcuDNN Archive â€‹ éœ€è¦æ³¨å†ŒNVIDIAè´¦å·ï¼Œæ ¹æ®åç§°ï¼Œæ‰¾å¯¹åº”åˆšæ‰cudaç‰ˆæœ¬çš„cudnnå®‰è£…åŒ…ä¸‹è½½è§£å‹ã€‚tar -xzvf cudnn-**-linux-x64-v**.tgzï¼Œwin10ä¸‹ä¸‹è½½cudnnæ—¶åç¼€å¯èƒ½è¢«ä¿®æ”¹ï¼Œé‡å‘½åä¸º**.tgzå³å¯ã€‚\n  è¿›è¡Œéƒ¨åˆ†æ–‡ä»¶çš„æ‹·è´ï¼Œä»cudnnè§£å‹åçš„cudaæ–‡ä»¶å¤¹ï¼Œå¤åˆ¶åˆ°ä¹‹å‰çš„cudaæ–‡ä»¶å¤¹ä¸­ï¼š\n  cp cuda/include/cudnn.h /path/to/your_cuda/include/\rcp cuda/lib64/libcudnn* /path/to/your_cuda/lib64\rchmod a+r /path/to/your_cuda/include/cudnn.h /path/to/your_cuda/lib64/libcudnn*\r æŸ¥çœ‹cudaå’Œcudnnå®‰è£…çŠ¶æ€ï¼šåˆ†åˆ«ä¸ºnvcc -Vå’Œ  cat /path/to/your_cuda/include/cudnn.h | grep CUDNN_MAJOR -A5\nå¦‚æœè¾“å‡ºæ­£å¸¸ç‰ˆæœ¬å·ï¼Œåˆ™è¯æ˜cudaå’Œcudnnå®‰è£…æˆåŠŸã€‚   nvcc -VæŸ¥çœ‹å®‰è£…çš„cudaç‰ˆæœ¬    æŸ¥çœ‹cudnnç‰ˆæœ¬  å¦‚æœcudaå®‰è£…æˆåŠŸåï¼Œå³nvcc -Væ˜¾ç¤ºæ­£ç¡®ç‰ˆæœ¬å·ï¼Œä½†æ˜¯cudnnå´å¤±è´¥äº†ï¼Œå¯ä»¥å°è¯•é€‰ç”¨ç¨æ—§ç‰ˆæœ¬çš„cudnnè€Œéæœ€æ–°ç‰ˆ\ntensorflowæˆªæ­¢åˆ°1.15ç‰ˆæœ¬ï¼Œéƒ½ä¸æ”¯æŒcuda-10.2ï¼Œä¸ºäº†å®‰è£…tensorflowï¼Œéœ€è¦æ‰‹åŠ¨é…ç½®cuda-10.0ï¼Œå¹¶ä¿®æ”¹~/.bashrcæ–‡ä»¶ä¸­çš„PATHå’ŒLD_LIBRARY_PATHå˜é‡ã€‚\nPyRenderå®‰è£… windowså®‰è£…ï¼šç¼ºå°‘freetypeä¾èµ–ï¼Œå‘ç°æ˜¯å› ä¸ºæ²¡æœ‰å®‰è£…visual studio\nPyTorch3Då®‰è£… PyTorch3Dæ˜¯Facebookå¼€æºçš„3Dè§†è§‰å·¥å…·åŒ…ï¼Œå®‰è£…æŒ‡å¼•ï¼šé“¾æ¥ windowsï¼ˆæ— cpuï¼‰å®‰è£…ï¼šä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼ˆéœ€è¦æå‰å®‰è£…å¥½torch, torchvisionç­‰å·¥å…·åŒ…ï¼‰\ngit clone https://github.com/facebookresearch/pytorch3d.git\rcd pytorch3d\rpip install -e .\r Linuxå¸¸ç”¨å‘½ä»¤ tmuxï¼šå°†ç¨‹åºæ”¾åœ¨åå°æ‰§è¡Œï¼Œå³ä½¿æ¨å‡ºsshç™»å½•ï¼Œåªè¦æœåŠ¡å™¨æ²¡æœ‰å…³æœºï¼Œç¨‹åºå°±ä¸ä¼šç»ˆç«¯ã€‚\nåˆ›å»ºæ–°çš„çª—å£ tmux new -s your_name\næš‚æ—¶é€€å‡ºå½“å‰çª—å£ ctrl+b+d\nç»“æŸç»ˆæ­¢å½“å‰çª—å£ exit\næŸ¥çœ‹å·²åˆ›å»ºçš„çª—å£ tmux ls\nè¿›å…¥ä¹‹å‰åˆ›å»ºçš„çª—å£ tmux attach -t your_nameæˆ–tmux a -t your_name\nnohupï¼šä¹Ÿæ˜¯å°†ç¨‹åºæ”¾åœ¨åå°æ‰§è¡Œï¼Œåœ¨æ²¡æœ‰tmuxçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨nohupæ˜¯ä¸€ä¸ªé€€è€Œæ±‚å…¶æ¬¡çš„é€‰æ‹©\næ‰§è¡Œä»»åŠ¡å¹¶æ”¾åˆ°åå° nohup bash yourscript.sh \u0026gt; your_log.log 2\u0026gt;\u0026amp;1 \u0026amp;\nä¹‹ååªèƒ½é€šè¿‡æŸ¥çœ‹logæ–‡ä»¶ï¼Œæ¥äº†è§£ç¨‹åºè¿è¡Œè¾“å‡ºæƒ…å†µã€‚ä¸èƒ½åƒtmuxé‚£æ ·è¿›å…¥çª—å£æŸ¥çœ‹ã€‚\ntopï¼Œhtopï¼Œps -auxï¼šè¿™ä¸‰ä¸ªéƒ½æ˜¯ç”¨æ¥æŸ¥çœ‹æœ¬æœºè¿è¡Œçš„ä»»åŠ¡çš„ï¼ŒæŒ‰éœ€å–ç”¨ã€‚htopå’Œps -auxå¯ä»¥æ¯”è¾ƒæ–¹ä¾¿çš„å®šä½åˆ°å…·ä½“çš„ä»»åŠ¡å¯¹åº”çš„PIDè¿›è¡Œï¼Œæ–¹ä¾¿è¿›è¡ŒKill\nscpï¼šç”¨æ¥è¿›è¡Œæ•°æ®ä¼ è¾“ã€‚\næœåŠ¡å™¨ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ï¼šscp username@server_ip_address:æ–‡ä»¶åœ¨æœåŠ¡å™¨çš„è·¯å¾„ æ–‡ä»¶åœ¨æœ¬åœ°çš„ä¿å­˜è·¯å¾„\næœ¬æœºæ–‡ä»¶ä¼ é€’ç»™æœåŠ¡å™¨ï¼š scp æ–‡ä»¶åœ¨æœ¬åœ°çš„ä¿å­˜è·¯å¾„ username@server_ip_address:æ–‡ä»¶åœ¨æœåŠ¡å™¨çš„è·¯å¾„\nè¿æ¥æœåŠ¡å™¨éœ€è¦æŒ‡å®šç«¯å£æ—¶ï¼ŒåŠ å…¥-P ï¼ˆå¤§å†™ï¼‰å‚æ•°\nnvidia-smiï¼Œgpustatï¼šè¿™ä¸¤ä¸ªå‘½ä»¤éƒ½å¯ä»¥ç”¨æ¥æŸ¥çœ‹å½“å‰æœºå™¨çš„æ˜¾å¡å ç”¨æƒ…å†µï¼Œgpustatéœ€è¦ä½¿ç”¨pipè¿›è¡Œå®‰è£…ï¼Œç•Œé¢ç®€æ´å¥½çœ‹äº›ã€‚\ntensorboardï¼šç”¨äºæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç»“æœ\nåŸºæœ¬è®¾ç½®ä¸ºï¼štensorboard --logdir path_to_log --port your_port\nä¹‹åç”¨æµè§ˆå™¨æ‰“å¼€ http://localhost:your_portå³å¯ï¼Œé»˜è®¤ç«¯å£6006\næ‰“å¼€æœåŠ¡å™¨ä¸Šçš„logæ–‡ä»¶\nè¿æ¥æœåŠ¡å™¨ï¼šssh -L æœ¬åœ°ç«¯å£:127.0.0.1:æœåŠ¡å™¨ç«¯å£ username@server_ip_address\nåœ¨æœåŠ¡å™¨ä¸Šæ‰§è¡Œï¼štensorboard --logdir path_to_log --port æœåŠ¡å™¨ç«¯å£\nåœ¨æœ¬åœ°æµè§ˆå™¨æ‰“å¼€ï¼šhttp://localhost:æœ¬åœ°ç«¯å£\ntensorboardçš„ä½¿ç”¨ï¼Œéœ€è¦pipå®‰è£…tensorboardåº“ï¼Œå¦‚æœå®‰è£…å®Œä¹‹åè¿˜æ˜¯æ²¡æœ‰ï¼Œéœ€è¦å¯»æ‰¾å¯¹åº”çš„æ–‡ä»¶\nalias tensorboard='python /home/username/anaconda3/envs/your_env_name/lib/python3.6/site-packages/tensorboard/main.py'\nwhich\nå¯ä»¥ç”¨äºæŸ¥çœ‹å½“å‰ä½¿ç”¨çš„å‘½ä»¤è·¯å¾„ï¼Œä¾‹å¦‚ï¼šwhich nvcc\nPyTorchç¬”è®°  å…³äºdevice   DataParallelä¼šè‡ªåŠ¨å°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°å¯¹åº”çš„æ¨¡å‹deviceä¸Šï¼ŒéªŒè¯ä»£ç ï¼š  def verify_device():\rclass net(nn.Module):\rdef __init__(self):\rsuper(net, self).__init__()\rdef forward(self, inputs):\rprint(\u0026quot;data's device in model: \u0026quot;, inputs.device)\rmy_net = net()\rmy_net = nn.DataParallel(my_net.cuda()) # my_net = my_net.cuda()\rinputs = torch.ones((16, 3, 256, 256))\rprint(\u0026quot;data's device out model: \u0026quot;, inputs.device)\rmy_net(inputs)\r åˆ©ç”¨DataParallel åªå°†æ¨¡å‹æ”¾åœ¨GPUä¸Šï¼Œè€Œæ²¡æœ‰å°†è¾“å…¥æ”¾åœ¨GPUä¸Šï¼Œè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š\ndata's device out model: cpu\rdata's device in model: cuda:0\rdata's device in model: cuda:1\rdata's device in model: cuda:2\rdata's device in model: cuda:3\rdata's device in model: cuda:5\rdata's device in model: cuda:6\rdata's device in model: cuda:7\rdata's device in model: cuda:4\r å¦‚æœä½¿ç”¨æ³¨é‡Šä¸­çš„my_net = my_net.cuda()ï¼Œåˆ™å¾—åˆ°\ndata's device out model: cpu\rdata's device in model: cpu\r BUGè®°å½•  PyTorchçš„dataloaderä¸€æ—¦æ‰§è¡Œ  for iter, batch in enumerate(dataloader):\nä¼šå¡ä½å¾ˆä¹…ï¼Œè¿›å…¥__getitem__è¿›è¡Œdebugå‘ç°ä¸€ç›´åœ¨è¯»å–å›¾ç‰‡ï¼Œè™½ç„¶æˆ‘ä½¿ç”¨çš„å¹¶ä¸æ˜¯opencvåº“è¿›è¡Œæ•°æ®è¯»å–\nè§£å†³åŠæ³•ï¼šåœ¨ä¸»ç¨‹åºä¸­åŠ å…¥cv2.setNumThreads(0)\nè°·æ­Œæœç´¢å…³é”®å­—ï¼špytorch dataloader stucks\nTensorflowå¤šæ¬¡loadæ¨¡å‹(ä¸ºäº†è¿›è¡Œå¤šæ¬¡æµ‹è¯•)æ—¶ï¼Œå¯èƒ½ä¼šå‡ºç°  ValueError: Variable *** already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? \nè¿™ç§æƒ…å†µï¼Œé€šå¸¸æ˜¯éœ€è¦åœ¨ä»£ç ä¸­åŠ å…¥ï¼Œæ¯æ¬¡loadæ¨¡å‹å‰æ¸…ç†ä¸€æ¬¡è®¡ç®—å›¾tf.reset_default_graph()\nåŸºäºPyTorchçš„ä»£ç ï¼Œè®­ç»ƒæ•ˆæœè¿˜ä¸é”™ï¼Œæµ‹è¯•æ•ˆæœå¾ˆç³Ÿç³•ï¼Œæ’é™¤è¿‡æ‹Ÿåˆçš„åŸå› ä»¥åŠæµ‹è¯•ä»£ç çš„é—®é¢˜ã€‚æœ€åå‘ç°æ˜¯è¯¥ä»£ç åœ¨æµ‹è¯•æ—¶éœ€è¦æŒ‡å®š  model.train()\nç¨‹åºæŠ¥é”™ï¼šcudnn_status_not_initialized  å¯èƒ½æ˜¯å› ä¸ºå½“å‰æ˜¾å¡æ­£åœ¨è¢«å…¶ä»–ä»»åŠ¡å ç”¨\nå®‰è£…neural_renderer_pytorch  RuntimeError: Error compiling objects for extension\nè§£å†³åŠæ³•ï¼šå°†pytorchçš„ç‰ˆæœ¬ä»1.5é™åˆ°1.2\nå¤šå¡GPUè®­ç»ƒï¼Œä¿å­˜çš„ä¹Ÿæ˜¯å¤šå¡æ¨¡å‹ï¼Œå´éœ€è¦å•å¡æµ‹è¯•  Unexpected key(s) in state_dict: \u0026quot;module.*\u0026quot;\nè§£å†³åŠæ³•ï¼š\nmodel.load_state_dict({k.replace('module.', ''): v for k, v in torch.load(model_state_path).items()})\næ¨¡å‹åœ¨GPUä¸Šè®­ç»ƒï¼Œéœ€è¦åœ¨CPUä¸Šæµ‹è¯•ï¼Œè·‘inference  è§£å†³åŠæ³•ï¼š\nckpt = torch.load(model_path, map_location=torch.device(\u0026quot;cpu\u0026quot;))\nç¨‹åºæŠ¥é”™: ModuleNotFoundError: No module named â€˜networkâ€™  åŸå› ï¼šä¿å­˜æ¨¡å‹æ—¶ä½¿ç”¨çš„æ˜¯torch.save(model) è€Œä¸æ˜¯torch.save(model.state_dict)\nè§£å†³åŠæ³•ï¼šåœ¨æ‰§è¡Œæ¨¡å‹ä¿å­˜å‘½ä»¤çš„pyæ–‡ä»¶è·¯å¾„ä¸‹ï¼Œå†æ¬¡è¯»å–æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨torch.save(model.state_dict)æ¥ä¿å­˜æ¨¡å‹\n","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607990400,"objectID":"299a8668a6985fe3bd61c6b0f83b582f","permalink":"http://xiaoqiangzhou.cn/post/code_notes/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/post/code_notes/","section":"post","summary":"Personal Code Notes","tags":["Coding","PyTorch"],"title":"Code Notes","type":"post"},{"authors":["Xin Ma","Xiaoqiang Zhou","Huaibo Huang","Gengyun Jia","Zhenhua Chai","Xiaolin Wei"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   --\rMethod:    Figure 1: Overall Framework  The self-supervised Siamese inference network consists of encoders $E_q$ and $E_k$. This inference network encodes the new key representations on-the-fly by using the momentum-updated encoder $E_k$. We insert the dual attention fusion module into several decoder layers, forming a multi-scale decoder. We allow the decoder to estimate the dense correspondence field (UV maps) and the feature maps that are used for the DAF module at multi-scales simultaneously. The inference network is firstly trained with contrastive learning. Then the pre-trained encoder $E_q$ and the decoder are jointly trained with the fusion module.\nExperiments:    Figure 2: Some qualitative results compared with state-of-the-arts on three datasets.  ","date":1638316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638316800,"objectID":"01f3f5ad02c8c45b32f57f9c625fa67a","permalink":"http://xiaoqiangzhou.cn/publication/pr-2021/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/publication/pr-2021/","section":"publication","summary":"Published in [*Pattern Recognition* (**PR**), 2021](https://www.sciencedirect.com/science/article/pii/S0031320321006415)","tags":["Image Inpainting","Contrastive Learning","Face"],"title":"Contrastive Attention Network with Dense Field Estimation for Face Completion","type":"publication"},{"authors":["Xin Ma","Xiaoqiang Zhou","Huaibo Huang","Zhenhua Chai","Xiaolin Wei","Ran He"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   --\rMethod:    Figure 1: Framework  The self-supervised Siamese inference network consists of encoder $E_q$ and $E_k$ with contrastive learning. This inference network encodes the new key representations on-the-fly by using the momentum-updated encoder $E_k$. We insert the dual attention fusion module into several decoder layers, thus forming a multi-scale decoder. The inference network is firstly trained on ImageNet with contrastive learning. Then the pre-trained encoder $E_q$ and the decoder are jointly trained with the fusion module.\nExperiments:    Figure 2: Some qualitative results compared with state-of-the-arts on three datasets.  ","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"dc01d2cbb165edbe6801190d9df72e18","permalink":"http://xiaoqiangzhou.cn/publication/icpr-2020-second/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/icpr-2020-second/","section":"publication","summary":"Published in [*International Conference on Pattern Recognition* (**ICPR**), oral paper, 2020](https://www.sciencedirect.com/science/article/pii/S0031320321006415)","tags":["Image Inpainting"],"title":"Free-Form Image Inpainting via Contrastive Attention Network","type":"publication"},{"authors":["Xiaoqiang Zhou","Junjie Li","Zilei Wang","Ran He","Tieniu Tan"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example.   --\rMethod:    Figure 1: Contrastive Relation Network  Firstly, features from each semantic sub-region are aggregated to a node representation and the nodes representation matrix is refined by a graph convolutional network. Then, the refined node representation is distributed to a feature map and used to modulate the corresponding features in the decoder network. The constrastive learning is adopted to faciliate the node representation learning.\nExperiments:    Figure 2: Some qualitative results compared with state-of-the-arts on three datasets.  ","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"477bef936862bb777da87e4deefa8c30","permalink":"http://xiaoqiangzhou.cn/publication/icpr-2020-first/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/icpr-2020-first/","section":"publication","summary":"Published in [*International Conference on Pattern Recognition* (**ICPR**), **Best Scientific Paper Award**, 2020](https://www.sciencedirect.com/science/article/pii/S0031320321006415)","tags":["Image Inpainting","Contrastive Learning"],"title":"Image Inpainting with Contrastive Relation Network","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne \r**Two** \rThree \r A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://xiaoqiangzhou.cn/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Xiaoqiang Zhou","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"6e8c048826a447efe7fc8fc75527cf08","permalink":"http://xiaoqiangzhou.cn/publication/conference-paper-2/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper-2/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"http://xiaoqiangzhou.cn/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]