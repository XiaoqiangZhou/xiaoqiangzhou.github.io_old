<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paper notes | Xiaoqiang&#39;s Homepage</title>
    <link>http://xiaoqiangzhou.cn/category/paper-notes/</link>
      <atom:link href="http://xiaoqiangzhou.cn/category/paper-notes/index.xml" rel="self" type="application/rss+xml" />
    <description>Paper notes</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 16 Dec 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://xiaoqiangzhou.cn/media/icon_hudd244c7a401d7898701885a05ebe1cd4_6259_512x512_fill_lanczos_center_3.png</url>
      <title>Paper notes</title>
      <link>http://xiaoqiangzhou.cn/category/paper-notes/</link>
    </image>
    
    <item>
      <title>How to Make a Benchmark Analysis</title>
      <link>http://xiaoqiangzhou.cn/post/benchmark/</link>
      <pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://xiaoqiangzhou.cn/post/benchmark/</guid>
      <description>&lt;h3 id=&#34;cvpr-2019-single-image-deraining-a-comprehensive-benchmark-analysis&#34;&gt;[CVPR 2019] Single Image Deraining: A Comprehensive Benchmark Analysis&lt;/h3&gt;
&lt;h6 id=&#34;motivation&#34;&gt;Motivation:&lt;/h6&gt;
&lt;p&gt;It is thus unclear how these algorithms would perform on rainy images acquired “in the wild” and how we could gauge the progress in the field.&lt;/p&gt;
&lt;h6 id=&#34;pipeline&#34;&gt;Pipeline:&lt;/h6&gt;
&lt;ol&gt;
&lt;li&gt;Formulate the rainy image synthesis process (from simple to complex)&lt;/li&gt;
&lt;li&gt;Summarize the contribution (thorough methods/testing sets)&lt;/li&gt;
&lt;li&gt;Method: Introduce the training sets&lt;/li&gt;
&lt;li&gt;Method: Introduce the testing sets&lt;/li&gt;
&lt;li&gt;Method: Introduce the additional task-driven (object detection in rainy images) testing sets.&lt;/li&gt;
&lt;li&gt;Experiments: Objective comparison (PSNR, SSIM, NIQE, SSEQ, and BLIINDSII)&lt;/li&gt;
&lt;li&gt;Experiments: Subjective comparison (11 human raters)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;tpami-2021-cross-domain-facial-expression-recognition-a-unified-evaluation-benchmark-and-adversarial-graph-learning&#34;&gt;[TPAMI 2021] Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning&lt;/h3&gt;
&lt;h6 id=&#34;motivation-1&#34;&gt;Motivation:&lt;/h6&gt;
&lt;ol&gt;
&lt;li&gt;Comprehensive and fair comparisons are lacking due to inconsistent choices of the source/target datasets and feature extractors.&lt;/li&gt;
&lt;li&gt;This journal paper is an extension work on their conference paper, which is published on ACM MM 2020. Therefore, the authors propose a novel method for this task.&lt;/li&gt;
&lt;/ol&gt;
&lt;h6 id=&#34;pipeline-1&#34;&gt;Pipeline:&lt;/h6&gt;
&lt;ol&gt;
&lt;li&gt;Category existing works by the training sets and backbone network.&lt;/li&gt;
&lt;li&gt;Build a simple baseline with ResNet-50 and take experiments to verify the influence of domain gap caused by the selected training sets.&lt;/li&gt;
&lt;li&gt;Conduct experiments to compare the effectiveness of different backbones under a baseline design.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Re-implement some SOTA methods manually and take thorough experiments with different training sets / backbone.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Evaluation Protocols: Dataset choice/Feature extractor choice&lt;/li&gt;
&lt;li&gt;Introduce the proposed method.&lt;/li&gt;
&lt;li&gt;Ablation study on the proposed method.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;tpami-2020-toward-bridging-the-simulated-to-real-gap-benchmarking-super-resolution-on-real-data&#34;&gt;[TPAMI 2020] Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution on Real Data&lt;/h3&gt;
&lt;h3 id=&#34;cvpr-2020-cross-domain-document-object-detection-benchmark-suite-and-method&#34;&gt;[CVPR 2020] Cross-Domain Document Object Detection: Benchmark Suite and Method&lt;/h3&gt;
&lt;h3 id=&#34;wacv-2021-adaptiope-a-modern-benchmark-for-unsupervised-domain-adaptation&#34;&gt;[WACV 2021] Adaptiope: A Modern Benchmark for Unsupervised Domain Adaptation&lt;/h3&gt;
&lt;h3 id=&#34;todo--the-first-work-that-defines-the-setting-of-domain-generalization-task&#34;&gt;[TODO ] The first work that defines the setting of domain generalization task&lt;/h3&gt;
&lt;h3 id=&#34;submitted-to-neurips-21-ood-bench-benchmarking-and-understanding-out-of-distribution-generalization-datasets-and-algorithms&#34;&gt;[submitted to NeurIPS 21] OoD-Bench Benchmarking and Understanding Out-of-Distribution Generalization Datasets and Algorithms&lt;/h3&gt;
&lt;h3 id=&#34;domain-generalization-in-vision-a-survey&#34;&gt;Domain Generalization in Vision: A Survey&lt;/h3&gt;
&lt;h3 id=&#34;number-of-paper-titles-including-benchmark&#34;&gt;Number of paper titles including &amp;ldquo;benchmark&amp;rdquo;:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;ICCV 2021:&lt;/em&gt; 18. &lt;a href=&#34;https://openaccess.thecvf.com/ICCV2021?day=all&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CVPR 2021:&lt;/em&gt; 17. &lt;a href=&#34;https://openaccess.thecvf.com/CVPR2021?day=all&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;ECCV 2020:&lt;/em&gt; 7.  &lt;a href=&#34;https://www.ecva.net/papers.php&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CVPR 2020:&lt;/em&gt; 6.  &lt;a href=&#34;https://openaccess.thecvf.com/CVPR2020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;TPAMI&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;TPAMI&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: &lt;b&gt;Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution on Real Data&lt;/b&gt;
   Authors: TPAMI 2020
&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;ICCV 2021&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models
   Authors: Jingjing Liu
2. Title: Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments
    Authors: Ziyi Bai, Xilin Chen
3. &lt;b&gt;Title: FloW: A Dataset and Benchmark for Floating Waste Detection in Inland Waters&lt;/b&gt;
   Authors:  &lt;b&gt;Yoshua Bengio&lt;/b&gt;
4. Title: Webly Supervised Fine-Grained Recognition: Benchmark Datasets and an Approach
   Authors: Jian Zhang, Heng Tao Shen
5. Title: H2O: A Benchmark for Visual Human-Human Object Handover Analysis
   Authors: Yanfeng Wang, Cewu Lu
6. Title: RobustNav: Towards Benchmarking Robustness in Embodied Navigation
   Authors:
7. Title: Towards Real-World Prohibited Item Detection: A Large-Scale X-Ray Benchmark
   Authors:
8. &lt;b&gt;Title: Transparent Object Tracking Benchmark&lt;/b&gt;
   Authors: Haibin Ling
9. Title: E-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks
   Authors:
10. &lt;b&gt;Title: Benchmarking Ultra-High-Definition Image Super-Resolution&lt;/b&gt;
   Authors: Hongdong Li, Ming-Hsuan Yang
11. Title: Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation
   Authors:
12. &lt;b&gt;Title: Real-World Video Super-Resolution: A Benchmark Dataset and a Decomposition Based Learning Scheme&lt;/b&gt;
   Authors: Lei Zhang
13. &lt;b&gt;Title: HDR Video Reconstruction: A Coarse-To-Fine Network and a Real-World Benchmark Dataset&lt;/b&gt;
   Authors: Lei Zhang
&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;CVPR 2021&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: &lt;b&gt;Multi-Shot Temporal Event Localization: A Benchmark&lt;/b&gt;
   Authors: Xiang Bai, Philip H. S. Torr
2. Title: &lt;b&gt;Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark Dataset and Baseline&lt;/b&gt;
   Authors:
3. Title: GMOT-40: A Benchmark for Generic Multiple Object Tracking
   Authors: Liang Lin
4. Title: ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis
   Authors: Ziwei Liu
&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;ECCV 2020&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: &lt;b&gt;Real-World Blur Dataset for Learning and Benchmarking Deblurring Algorithms&lt;/b&gt;
   Authors: Jucheol Won, Sunghyun Cho
2. Title: &lt;b&gt;Towards causal benchmarking of bias in face analysis algorithms&lt;/b&gt;
   Authors: MIT, Amazon
&lt;/pre&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;&lt;i&gt;CVPR 2020&lt;/i&gt;&lt;/summary&gt;
&lt;pre&gt;
1. Title: &lt;b&gt;Cross-Domain Document Object Detection: Benchmark Suite and Method&lt;/b&gt;
   Authors: Yun Fu
2. Title: &lt;b&gt;Supervised Raw Video Denoising With a Benchmark Dataset on Dynamic Scenes&lt;/b&gt;
   Authors: Ronghe Chu, Jingyu Yang
&lt;/pre&gt;
&lt;/details&gt;
</description>
    </item>
    
    <item>
      <title>[AAAI 2022] Revisiting Global Statistics Aggregation for Improving Image Restoration</title>
      <link>http://xiaoqiangzhou.cn/post/chu_revisiting/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>http://xiaoqiangzhou.cn/post/chu_revisiting/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/0_hubcb6207f291c8349ce1e6771be420ded_26203_50551a23622d6ba605ce93a2febed507.PNG 400w,
               /media/posts/Chu_revisiting/0_hubcb6207f291c8349ce1e6771be420ded_26203_655c2cf608a8f82a32a6ab53f2a4a0ad.PNG 760w,
               /media/posts/Chu_revisiting/0_hubcb6207f291c8349ce1e6771be420ded_26203_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/0_hubcb6207f291c8349ce1e6771be420ded_26203_50551a23622d6ba605ce93a2febed507.PNG&#34;
               width=&#34;760&#34;
               height=&#34;124&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;summary&#34;&gt;Summary:&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Domain gap between training/testing statistics distribution. This paper shows that statistics aggregated on the patches-based/entire-image-based feature in the training/testing phase respectively may distribute very differently.&lt;/li&gt;
&lt;li&gt;The test-time solution can be summarized as converting the statistics aggregation operation from global to local, i.e. each pixel of the feature aggregates its own statistics locally.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;method&#34;&gt;Method:&lt;/h4&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-figure-1-illustration-of-training-and-testing-schemes-of-image-restoration&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/1_hucbeb4ea3d14f171cb4bad5478aff1a37_189404_fcf6783fd6dca99fdc5245eadc7f98c3.PNG 400w,
               /media/posts/Chu_revisiting/1_hucbeb4ea3d14f171cb4bad5478aff1a37_189404_7996b112f4293090ff5808333d721e00.PNG 760w,
               /media/posts/Chu_revisiting/1_hucbeb4ea3d14f171cb4bad5478aff1a37_189404_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/1_hucbeb4ea3d14f171cb4bad5478aff1a37_189404_fcf6783fd6dca99fdc5245eadc7f98c3.PNG&#34;
               width=&#34;444&#34;
               height=&#34;505&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1: Illustration of training and testing schemes of image restoration
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The computational complexity of local statistics for the whole image can be reduced from $O(HWK_hK_w)$ to $O(HW)$ with some mathematical tricks.
















&lt;figure  id=&#34;figure-figure-2-formulation-of-the-global-statistics-calculation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/2_hua655102713792929bb891bd08a988625_6675_245ad170589f238ce8ffb08eabaa987a.PNG 400w,
               /media/posts/Chu_revisiting/2_hua655102713792929bb891bd08a988625_6675_a766e4dccb6b8ea7b599f379155e73e1.PNG 760w,
               /media/posts/Chu_revisiting/2_hua655102713792929bb891bd08a988625_6675_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/2_hua655102713792929bb891bd08a988625_6675_245ad170589f238ce8ffb08eabaa987a.PNG&#34;
               width=&#34;267&#34;
               height=&#34;66&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 2: Formulation of the global statistics calculation
    &lt;/figcaption&gt;&lt;/figure&gt;

















&lt;figure  id=&#34;figure-figure-3-formulation-of-the-local-statistics-calculation&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/3_hudfd7a9fccad84aae8d319dbc232eca76_6472_883f35f5566ea6f13dc276451848d99a.PNG 400w,
               /media/posts/Chu_revisiting/3_hudfd7a9fccad84aae8d319dbc232eca76_6472_66905c017bae1f8dcbf5e5c331759500.PNG 760w,
               /media/posts/Chu_revisiting/3_hudfd7a9fccad84aae8d319dbc232eca76_6472_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/3_hudfd7a9fccad84aae8d319dbc232eca76_6472_883f35f5566ea6f13dc276451848d99a.PNG&#34;
               width=&#34;303&#34;
               height=&#34;58&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 3: Formulation of the local statistics calculation
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;The authors extend the proposed modules to the  Squeeze and Excitation (SE) block and Instance Normalization (IN) block.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;experiments&#34;&gt;Experiments&lt;/h4&gt;
&lt;p&gt;The authors take experiments on image restoration task and semantic segmentation task.
















&lt;figure  id=&#34;figure-figure-4--visualization-of-the-statistics-mean-distribution-of-in-and-se-in-traintest-time&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;screen reader text&#34; srcset=&#34;
               /media/posts/Chu_revisiting/4_huacd4ec92d12c6dd417289ba5c18477d9_102123_f4aace952048351c3036dce33503929a.PNG 400w,
               /media/posts/Chu_revisiting/4_huacd4ec92d12c6dd417289ba5c18477d9_102123_2b08cb8e73d241b2bc6805d023211817.PNG 760w,
               /media/posts/Chu_revisiting/4_huacd4ec92d12c6dd417289ba5c18477d9_102123_1200x1200_fit_lanczos_3.PNG 1200w&#34;
               src=&#34;http://xiaoqiangzhou.cn/media/posts/Chu_revisiting/4_huacd4ec92d12c6dd417289ba5c18477d9_102123_f4aace952048351c3036dce33503929a.PNG&#34;
               width=&#34;760&#34;
               height=&#34;549&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 4:  Visualization of the statistics (mean) distribution of IN and SE in train/test-time
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id=&#34;some-take-away-conclusions&#34;&gt;Some take-away conclusions&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Full-image training causes severe performance loss in low-level vision task.  This is explained by that full-images training lacks cropping augmentation&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
